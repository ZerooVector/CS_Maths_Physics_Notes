#MA 

## 特征值分解
在几何上，特征值分解与一维不变子空间有关：
![[Pasted image 20230303094725.png|600]]
在数学上，我们这样表示特征值分解：
$$
AV = V \Lambda\quad\Lambda = \mathrm{diag}(\lambda_i)
$$
也就是说，在入口基向量和出口基向量相同——都是矩阵的特征值的情况下，线性变换 $\Lambda$ 仅仅表示对于向量的一个伸缩，而不会使得其方向发生变化。

## 对角化
我们将特征值分解的式子变化一下，就得到：
$$
A = V\Lambda V^{-1}
$$
由于 $\Lambda$ 是一个对角矩阵，所以我们称 $A$ 是可对角化的。
利用矩阵的对角化，可以快速计算举报 v 真的高次幂，这在 Markov 链里面可能很有用：
$$
A^n = V\Lambda^nV^{-1} = V\mathrm{diag}{(\lambda_1^n,\lambda_2^n,\cdots,\lambda_n^n)}V^{-1}
$$

## 谱分解
如果 $A$ 为对称矩阵，那么 $V^{-1} = V^T$，我们再将特征值分解的式子变化一下，就得到：
$$
A = V\Lambda V^T
$$
这时，称矩阵 $A$ 合同到一个矩阵 $\Lambda$ ，那么 $A$ 的特征值分解就可以写成：
![[Pasted image 20230303151447.png|400]]
我们知道：
$$
\sum_i v_i \otimes v_i  = VV^T = I 
$$
这种分解将一个**对称矩阵**拆解成**一系列的特征值和特征向量的张量积之和**，每一个 $v_i \otimes v_i$ 就是一个“谱线”，各个谱线的强度被叠加，因此，这种分解称作谱分解。

我们也可以把两个特征向量拼成的矩阵放到原矩阵 $A$ 的一侧：
$$
V^TAV = \Lambda 
$$
这个时候将式子展开：
![[Pasted image 20230303151922.png]]
那么，可以注意到：
$$
v_j^TAv_j = \lambda_j \qquad v_i^TAv_j = 0(i \not = j)
$$

### 例子：Gram 矩阵的谱分解
Gram 矩阵被定义为：
$$
G = X^TX
$$
那么这个矩阵是对称阵，可以进行谱分解（这个时候，谱分解的式子已经被反过来了）：
![[Pasted image 20230303152535.png|450]]

我们记 $y_i = Xv_i$，这代表着数据矩阵 $x$ 向着其特征向量上的投影，那么，谱分解被写成：
![[Pasted image 20230303152831.png|400]]

那么，我们发现——特征值（即对角元）可以写成向量 $y$ 的二范数的形式：
$$
||y_j||^2 = ||Xv_j||^2 = \lambda_j
$$
![[Pasted image 20230303153443.png|400]]
如图，展示了所有数据向着第一个特征向量的方向投影的情形，那么
$$
||y_j||^2_2 = \sum_i(y_j^{(i)})^2
$$
这个二范数就是所有投影点到原点距离的平方和。这也就意味着——**Gram 矩阵的每一个特征值，就是所有矩阵往特征向量方向投影之后，与原点距离的平方之和**。所有非对角元都是 0，这是因为对称矩阵的特征向量两两正交（==仔细想想，这话可能不对==）。
“反过来”的谱分解中的 $V^TGV$ 中的每一个元素都包含着数据矩阵向着每一个特征值方向的投影，“正着来”的谱分解则就是很正常的意义——数据分解成“光谱”，再叠加。比如：
![[Pasted image 20230303154224.png]]
（注意：这里不应该是矩阵乘法，应该是相加）

## 特征值的诸多性质
### 特征值的一般性质
我们之前谈过行列式的直观意义——以矩阵的各个列向量为边，张成的几何体的体积就是行列式的值。我们通过运算，可以发现——相似对角化前后，矩阵的行列式的值不会变（==如何理解这一点？==）这说明，相似对角化在几何直观上的理解是将一个图形“拉正了”。
![[Pasted image 20230303154859.png|400]]

那么，特征值有如下的性质：
- $A$ 的标量积 $kA$ 的特征向量不变，特征值为 $k\lambda$
- $A^n$ 的特征向量不变，特征值为 $\lambda^n$，这可以被推广为：$A^n V = V\Lambda^n$
- 若 $A$ 可逆，则 $A^{-1}$ 的特征向量仍然不变，特征值为 $1/\lambda$ 
- $A$ 的行列式是“长方体“各个边的长度的乘积，也就是各个特征值的乘积：$\mathrm{det}(A) = \prod_j \lambda_j$ 
- $A (n\times D)$ 的标量积 $kA$ 的行列式为：$\mathrm{det}(kA) = k^D\prod_j\lambda_j$ 
- $A$ 的迹 (trace) 是特征值之和 $\mathrm{tr}(A)  = \sum_i \lambda_i$

### 复数特征值
如果 $A$ 的特征方程 $|A-\lambda I| = 0$ 求解得到了附属跟，那么就是复数特征值。复数特征值总是以共轭复数的形式成对地出现。我们先假设 $A$ 是对称矩阵（这样最简单）


### 最大特征值——放大 or 缩小
最后，我们再来看看，能不能从特征值中读出一些关于矩阵代表的线性变换的信息：根据
$$
A = V\Lambda V^{T}
$$
我们把它展开，正常情况下，会写成很多张量积叠加的形式：
$$
\begin{aligned}
Ax &= V\Lambda V x\\
 &= [v_1,v_2,\cdots,v_n]\mathrm{diag}(\lambda_1,\lambda_2,\cdots,\lambda_n)[v_1,v_2,\cdots,v_n]^Tx\\
  &= \sum v_i\lambda_i(v_ix)
\end{aligned}
$$
那么，这样相当于先将要变换的向量 $x$ 投影到单位向量 $v_i$ 上，再按照特征值放大/缩小。由于各个 $v_i$ 又是互相正交的。所以其实我们将 $A$ 对 $x$ 的变换拆解到了各个特征值的方向上。那么哪个特征值最大，这个方向上的变换也越显著。记矩阵的最大特征值为 $\lambda_{\max}$ 如果 $\lambda_{\max}>1$，则 $x$ 会越变越长；否则反之。对于非对称的矩阵，也有类似的结果。
![[Pasted image 20230303161447.png|400]]





#MA 
## SVD 的几何视角
对一个 $X_{n\times D}$ 进行 SVD，得到：
$$
X_{n\times D} = USV^T
$$
其中，$U$ 的列向量称为“左奇异向量”；$V$ 的列向量称为“右奇异向量”，$U$ 和 $V$ 都是正交矩阵。因此，$U = [u_1, u_2,\cdots,u_n]$ 是 $\mathbb R^n$ 上的规范正交基，$V = [v_1, v_2,\cdots,v_D]$ 是 $\mathbb R^D$ 上的规范正交基。
因此，从几何上来说，$U$ 和 $V$ 的作用是旋转，而 $S$ 的作用则是缩放。
下面演示了一个二维空间输入，二维空间输出的 SVD：
![[Pasted image 20230306084631.png|400]]

## 不同类型的 SVD
完全型 SVD 是这样的：
![[Pasted image 20230306084955.png|500]]
但是，我们发现，在计算矩阵乘法 $U\times S$ 时，$U$ 的最右侧两列根本没有被使用到。我们可以直接删去这两列，这样的分解成为经济型 SVD：
![[Pasted image 20230306085318.png|500]]
这个时候，$S$ 成为一个对角阵。

## 特征值和奇异值的关系
我们知道，进行 SVD 的矩阵的形状通常是不规则的，因此，我们是借助特征值分解来计算 SVD 的结果的，我们会对两个 Gram 矩阵进行特征值分解，例如：
$$
\begin{aligned}
XX^T &= P\Lambda P^T\\
& = (USV^T)(USV^T)^T\\
&= USV^TTS^TU^T\\
&= USS^TU^T
\end{aligned}
$$
那么，注意到第一行的 $P\Lambda P^T$ 和最后一行的 $USS^TU$ 实际上是同构的，因此，$U$ 里面装的就是 $XX^T$ 的特征向量，$SS^T$ 的对角元是特征值；而原来，$S$ 的对角元是奇异值。利用 $SS^T = \Lambda$，容易找到奇异值和特征值的关系是
$$
\lambda_j = s_j^2
$$
从形式上，我们很容易将 SVD 和 QR 分解进行类比：
$$
X= QR
$$
$$
S = U(SV^T)
$$
显然，$Q$ 和 $U$ 都是正交矩阵，都代表了一组规范正交基。但是，$Q$ 的第一个基是与 $X$ 的第一列平行的，而 SVD 则是逐步选取尽可能大的奇异值。这一视角将在后续被介绍。
对于另一边的 Gram 矩阵进行特征值分解，也可以得到类似的结论：
$$
X^TX=V(S^TS)V^T
$$
## 从两个不同的视角理解 SVD
### SVD 是投影
我们考虑 $n\times D$ 的矩阵参与经济型 SVD 分解，也就是：
$$
X = USV^T
$$
由于 $V^TV=I$，我们看看能不能把 $V$ 拿到左边来： 在等式两侧乘以 $V$，得到：
$$
XV = US
$$
![[Pasted image 20230306091043.png|400]]

将这个式子打开，得到：
$$
X[v_1,v_2,\cdots,v_D] = [u_1,u_2,\cdots,u_D]\mathrm{diag}(s_1,s_2,\cdots,s_D)
$$
拿出其中的一个运算：
$$
Xv_i  = u_is_i
$$
这也就是说，以 $v_j$ 为系数，$X$ 的列向量的线性组合（或数据矩阵 $x$ 向一个向量 $v_i$ 上的投影），运算结果为 $u_is_i$。这里，$v_i$ 和 $u_i$ 都是单位向量，那么：
$$
||Xv_i|| = ||u_is_i|| = s_i
$$
也就是说，$X$ 向着 $v_i$ 上的投影长度仅仅取决于奇异值 $s_i$ 的大小，由于奇异值是从大到小排列的，所以 $X$ 的几个投影的重要性也是从大到小排列的。


### SVD 是叠加
直接展开 SVD 就可以看到 SVD 的叠加效果：
![[Pasted image 20230306092245.png|500]]
其中的每一项都是一个张量积，那么数据矩阵 $X$ 就是各个张量积的加权和。注意：每一个张量积的秩都是 1，因此，叠加了几项，最后的秩就会是几。我们通常说，SVD 解决的是**低秩估计**的问题。
![[Pasted image 20230306092653.png|400]]

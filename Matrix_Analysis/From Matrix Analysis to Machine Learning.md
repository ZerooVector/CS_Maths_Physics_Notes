#MA  

## 随机变量的线性变换 
我们用线性代数语言重构一些随机变量的线性变换。为了方便书写，以二元随机变量为例
如果 $Y$ 是由两个随机变量组合而来的，也就是说：
$$
Y = [a \ b]\begin{bmatrix}X_{1} \\ X_{2}\end{bmatrix}
$$
期望的变换：
$$
E(Y) = [a\ b] \begin{bmatrix}E(X_{1})  \\ E(X_{2})\end{bmatrix}
$$
方差的变换：
$$
\mathrm{var}(Y) = [a\ b] \Sigma \begin{bmatrix}a \\ b\end{bmatrix}
$$
高维随机变量的情况：
考虑 $D$ 维的随机变量 $\zeta = [z_{1}, z_{2},\cdots ,z_{D}]^{T}$ 服从多元高斯分布，也就是说，$\mu_{\zeta} = [0,0,\cdots ,0]^{T},\mathrm{var} = \mathrm{diag}(1,1,\cdots,1)$ ，我们使用一个矩阵 $V$ 表征对各个随机变量进行线性组合，使用向量 $\mu= [\mu_{1},\mu_{2},\cdots,\mu_{n}]$ 对均值进行偏移，那么我们得到一个多维随机变量：
$$
\chi  = \begin{bmatrix}x_{1} \\ x_{2} \\ \cdots  \\ x_{n}\end{bmatrix} = V^{T}\begin{bmatrix}z_{1} \\ z_{2} \\ \cdots \\ z_n\end{bmatrix}+\begin{bmatrix}\mu_{1} \\ \mu_{2} \\ \cdots \\ \mu_{n}\end{bmatrix}
$$
$V$ 的一列代表着对 $z_{i}$ 的一种线性组合，那么我们可以推导 $\chi$ 的协方差矩阵：
$$
\begin{align*}
 \mathrm{cov}(\chi ,\chi )\\
&= \mathrm{E}(\frac{(\chi -\mu_{\chi} )(\chi - \mu_{\chi} )^{T}}{n})\\
 &=\mathrm{E}( V^{T}\frac{\zeta\zeta^{T}}{n}V)\\
&= V^{T}I V \\
 &= V^{T}V
\end{align*}
$$
那么，我们推广到一般情况，对于 $n$ 维的随机变量 $\chi$ 和 $\gamma$ 之间满足一个线性映射关系：
$$
\gamma = A\chi 
$$
那么质心：
$$
\mu_{\gamma} = A \mu_{\chi}
$$
协方差：
$$
\mathrm{var}(\gamma) = \Sigma_{\gamma}  = A \Sigma_{\chi} A^{T}
$$
## 数据矩阵向着指定列向量的投影
这个投影被定义为：
$$
y  =Xv = \sum x_{i}v_{i}
$$
其中，$x_{i}$ 是 $X$ 矩阵的一个列向量，那么，从几何直观意义上说，这就是将 $X$ 的各个列向量线性组合起来。

我们接下来计算投影 $y$ 的期望和方差：
期望是显然的，因为经过简单的运算换顺序就可以证明，期望的投影等于投影的期望：
$$
E(y) = E(x_{j})v
$$
方差不是显然的：
$$
\begin{align*}
\mathrm{var}(y) &= \frac{(y-\mathrm{E}(y))^{T}(y-\mathrm{E}(y))}{n-1}\\
 &= \frac{(Xv - \mathrm{E}(X)v)^{T}(Xv - \mathrm{E}(X)v)}{n-1}\\
 &= v^{T} \frac{(X-E(X))^T(X-E(X))}{n-1}v\\
 &= v^{T}\Sigma_{X}v
\end{align*}
$$
其实，这里的 $\Sigma_{X}$ 是一个（物理意义上的）张量。至此，我们解释了协方差矩阵的投影

## 线性回归
线性回归的实质是解决不相容方程组 $Xb = y$ 的求解问题，这个方程组不能求解的原因是 $y$ 落在了 $X$ 的各个列向量张成的超平面之外。
那么，利用残差向量垂直于超平面，我们显然得到：
$$
X^{T} (y-Xb)= 0  \Rightarrow b  (X^{T}X)^{-1}X^{T}y
$$
利用 SVD 分解和 QR 分解，可以将 $b$ 整理成不同的形式：
![[Pasted image 20230312121044.png|400]]

![[Pasted image 20230312121327.png|400]]

那么，显然，$U$ 可以视作 $Q$，因为二者都是正交矩阵；$SV^T$ 可以视作 $R$ 

线性回归也可以视作一个优化问题求解：
$$
\begin{align*}
 \min \epsilon_{i}^{2}\\
&= \epsilon^{T}\epsilon\\
 &= (y-Xb)^{T}(y-Xb)\\
&= y^{T}y - y^{T}Xb-b^{T}X^{T}y +b^{T}X^{T}Xb\\
 &= y^{T}y -y^{T}Xb - y^{T} X b+ b^T X^{T}Xb\\

\end{align*}
$$
其中，利用到了式子中的四项都是标量，转置不影响计算结果的性质。
那么，原问题转化为一个优化问题：
$$
f(b) = y^{T}y -2y^{T}Xb +b^{T}X^{T}Xb
$$
一阶导：
$$
\frac{\partial f(b)}{\partial b} = -2X^{T}y+2X^{T}Xb = 0
$$
这可以得到和上面一样的结果。
此外，目标函数的二阶导等于 $2X^{T}X$，若 $X$ 列满秩，则 $X^T X$ 正定，这保证了我们求到的是最小值。

## 数据矩阵向着多个指定方向的投影
之前我们知道，数据矩阵向着 $v$ 的投影是 $Xv$，那么，我们只要将多个向量组成矩阵 $V = [v_{1},v_{2},\cdots ,v_{n}]$，$XV$ 的结果就是将数据矩阵 $X$ 向着多个指定的向量 $[v_{1},v_{2},\cdots ,v_{n}]$ 的方向投影，每一列都是一个投影的结果。
如果数据矩阵和投影之间满足以下关系：
$$
Y = XV
$$
那么，投影的期望和原来的期望满足：
$$
E(Y) = E(X) V
$$
投影的协方差和原来的协方差满足：
$$
\Sigma_{Y} = V^T \Sigma_{X}V
$$

## PCA
有许多种方法可以最后给出主成分分析。比如对中心化数据的 Gram 矩阵特征值分解。这里再给出一种方法：
![[Pasted image 20230312154541.png]]

有六种可能得到正交基的方法，但是有些方法得到的正交基不一定是对的。
![[Pasted image 20230312154935.png]]
![[Pasted image 20230312154942.png]]

在笔者的印象里（根据本书前文的论述），只有对中心化矩阵和协方差矩阵的操作才是正确的。后面会进行验证。

---

《机器学习中的矩阵分析》是由数学家们共同研发的一款全新开放世界冒险游戏。游戏发生在一个被称作「线性空间」的幻想世界，在这里，被神选中的人将被授予「向量和矩阵」，导引矩阵运算之力。你将扮演一位名为「大学生」的神秘角色，在困难重重的探索中邂逅性格各异、能力独特的矩阵分解和投影们，和他们一起击败强敌，找回失散的成绩——同时，逐步发掘「数据矩阵」的真相。


--- 




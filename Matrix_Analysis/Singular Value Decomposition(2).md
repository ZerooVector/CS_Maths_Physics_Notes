#MA 

## SVD 的全部四种形态
### 完全型：正常的 SVD 分解
![[Pasted image 20230306094350.png|500]]

### 经济型：把 $S$ 矩阵中的零行去掉，$U$ 矩阵中用不到的列去掉
![[Pasted image 20230306094645.png|500]]

### 紧凑型：仅对于非满秩的数据矩阵可行
一个 $n\times D$ 的矩阵（通常来说 $n<D$），其最大的秩为 $D$。如果秩不足 $D$，那么矩阵不为 0 的奇异值只能有 $r$ 个，这意味着 $S$ 中又会出现空行。我们删除多余的行，以及 $U$ 阵中多余部分即可。
![[Pasted image 20230306094926.png|500]]

### 截断型：是对原矩阵的低秩估计
我们知道，SVD 是一个层层叠加，还原原始数据的过程。但是如果我们不用那么多层，就可以得到原始数据的近似解：
![[Pasted image 20230306095149.png|500]]

## 利用 SVD 还原数据，SVD 与 PCA的关系
我们已经知道，SVD 可以写成层层叠加的形式：
$$
X = USV^T = \sum_i s_iu_iv_i^T = \sum_i s_iu_i\otimes v_i
$$
又因为矩阵 $V$ 的列向量和 $U$ 的列向量有一定的关系：
$$
X=USV^T \Rightarrow XV = US \Rightarrow Xv_i = s_iu_i
$$
以上两式联立：
$$
X = \sum_i Xv_i \otimes v_i
$$
这和我们之前看到的 PCA 非常相像：[[Data Projection and PCA]]
这就是找到了几个向量 $v_i$（就是矩阵 $V$ 的列向量），然后将矩阵 $X$ 向着 $v_i$ 上投影，$Xv_i$ 就是主成分，再做张量积，意味着将主成分打回原来的坐标轴上。因此，我们就找到了 PCA 和 SVD 之间的关联。

## 利用 SVD 做数据的低秩估计
显然，只要取前几个主成分就能显著地展示数据的特征，因此，如果我们用前几个奇异值还原数据，就可以对数据做出估计。

## 数据的正交化
我们知道，取很多个正交的列向量 $v_i$，把它们拼成正交矩阵 $V$，那么
$$
Z = XV
$$
意味着数据 $X$ 向着矩阵 $V$ 的各个列向量上投影，得到的 $Z$ 的一横行就意味着 $X$ 在各个 $v_i$ 上的坐标。那么我们现在将这一组规范正交基取为 SVD 分解中的 $V$ 矩阵的各列（由于 PCA 实际上就是向着 $V$ 的各个列上投影，所以这样做计算得到的就是**每组数据的主成分**），我们考虑将 $X$ 写成 SVD 分解的形式：
$$
Z = USV^TV  = US = [s_1u_1,\cdots,s_nu_n]
$$
也就是说：
$$
[z_1,z_2,\cdots,z_D] = [s_1u_1,\cdots,s_Du_D]
$$
也就是说，在“经济型”的 SVD 分解中，$U$ 矩阵的各个列是一个特征经过投影后“单位化”的结果，乘上对应的特征值就出现了主成分矩阵的一列。这也给了我们快速计算主成分的方法。

因此，我们用下图概括 SVD 和 PCA 的关系：
![[Singular Value Decomposition(2) 2023-03-07 13.07.31.excalidraw|500]]



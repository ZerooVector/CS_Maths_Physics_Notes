#MA  

首先，我们列出所有矩阵分解的总结：
![[Pasted image 20230312095315.png]]
![[Pasted image 20230312095408.png]]
显然，奇异值分解是最特殊的情形。将其特殊化到方阵上，得到的就是特征值分解。如果 $A$ 又是对称矩阵，那么就可以执行谱分解。

以及对于一些常用的矩阵分解如下图：
![[Pasted image 20230312100239.png]]
![[Pasted image 20230312100250.png]]

现在，我们开始列举每一种矩阵分解的作用。

## QR 分解：获得数据的正交系
![[Pasted image 20230312101258.png|400]]
如图，QR 分解的 Q 矩阵给出数据的一组正交坐标，而 R 矩阵给出组合的系数

## Cholesky 分解：找到高维列向量在低维空间中的坐标
### 在 Gram 矩阵上执行
我们知道，Gram 矩阵被定义为：
$$
G = X^{T}X = (<x_{i},x_{j}>)
$$
这里的 $x_{i},x_{j}$ 都是 150 维的，但是我们又知道，这些向量之间显然线性相关（只有 $D$ 个彼此线性无关的向量），所以我们能不能在 $D$ 维中找到这些向量的坐标呢？考虑进行 Cholesky 分解：
$$
G = R_{G}^{T}R_{G} = [r_{G,1},r_{G,2},\cdots,r_{G,D}]  \begin{bmatrix}r_{G,1} \\ r_{G,2} \\ \cdots  \\ r_{G,D}\end{bmatrix}
$$
容易看出，这样的乘法和上面 $G = X^{T}X$ 写成了完全相同的形式，矩阵中的每一项可以是 $<r_{G,i},r_{G,j} >$，但是，这个时候，所有的 $r_{G}$ 向量都是 $D$ 维的，我们相当于用 $D$ 维上两个向量的内积给出了 Gram 矩阵。这些 $r_G$ 向量就可以看作是原始数据向量在 $D$ 维上的坐标。

### 在协方差矩阵上执行
![[Pasted image 20230312103757.png]]
此时，你可以得到：
$$
\Sigma = R_\Sigma^{T}R_{\Sigma}= (<\sigma_{i},\sigma_{j}>)
$$
其中，$\sigma_{i}$ 是一个 $D$ 维上的向量，我们可以称它为标准差向量。它的模长是标准差，而两个向量间的夹角是 Pearson 相关系数。此外，根据协方差矩阵的定义，这组向量就是一组以数据质心为原点的非正交基底。


## 特征值分解：获得行空间、零空间和正交的基底
### 在 Gram 矩阵上执行
![[Pasted image 20230312104251.png]]
根据 SVD 的计算过程可以知道，这里的 $V_{X}$ 实际上是就是 SVD 中的 $V$，那么，$V$ 的各个列其实包含了 $X$ 的行空间的一组正交基，以及 $X$ 零空间的一组正交基

#### 从优化视角上看
实际上，SVD 是希望使得 $X$ 在特定方向上的投影的模长最大，找到一个单位向量 $v$，$X$ 在 $v$ 上的投影是 $Xv$，那么模长就是
$$
v^{T}Gv = v^{T} X^{T}Xv= ||y||^{2}
$$
$y$ 的模长的最大值就是 $s_{1}$（最大的奇异值）

#### 性质
所有特征值之和等于 $G$ 的 trace，也就是说：
$$
\sum_{i}\lambda_{i}= \mathrm{tr}(G) = \sum_{i}||x_{i}||^2
$$
### 在协方差矩阵上执行
现在，优化的目标就变成了：
$$
v^{T}(n-1) \Sigma v = v^{T} X_{c}^{T }X_{c}v = (n-1)\mathrm{var}(y_{c})
$$
也就是要最大化数据在投影方向上的方差
类似于上面的性质，我们也有：
$$
\sum_{i}\lambda_{i} = \mathrm{tr}(\Sigma) = \sum_{i}||x_{ci}||^{2} = \sum_{i}\sigma_{i}^{2}
$$
相关性矩阵上是标准化后的协方差，和协方差矩阵有类似的性质

## SVD 分解：一次性获得四个线性空间
原始数据、中心化数据、Z-score 数据都可以进行 SVD 分解，分解后可以得到行（列）空间，左（右）零空间。
![[Pasted image 20230312111148.png]]

![[Pasted image 20230312111447.png]]



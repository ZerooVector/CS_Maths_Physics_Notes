#MA  
拉格朗日乘子法有其历史渊源：在物理学中，如果约束是非完整的 (例如，一个圆盘在二维平面上滚动，这显然是一个典型的非完整约束的问题)，这样的问题无法使用第二类拉格朗日方程，只能使用第一类拉格朗日方程（乘子法）来处理。因此，乘子法的作用就是破解一些“奇怪”的约束条件。
一个最小化问题的基本形式是：
$$
\begin{align*}
\arg \min_{x}f(x)\\
s.t.\ \ \  l &\leq x \leq u \\
Ax &\le B\\
A_{eq} x &= b_{eq}\\
c(x) &\le 0\\
c_{eq}(x) &= 0
\end{align*}
$$
五个约束条件的含义分别是：
- 决策变量的上下界
- 线性不等式约束
- 线性等式约束
- 非线性不等式约束
- 非线性等式约束

## 拉格朗日乘子法的几何意义
### 只有等式约束的情况
我们先来考虑最简单的情况：只有等式约束。也就是说：
$$
\begin{align*}
\min_{x}f(x) \\
s.t. \ \ \ h(x) &= 0
\end{align*}
$$
我们的做法是：构造下面的拉格朗日函数：
$$
L(x,\lambda) = f(x)+\lambda h(x)
$$
极值条件是对 $x,\lambda$ 的偏导都为 0. 也就是说
$$
\begin{align*}
\nabla_{x}L(x,\lambda) &= \nabla f(x)+\lambda\nabla h(x) = 0\\
\nabla_{\lambda}L(x,\lambda ) &= h(x) = 0
\end{align*}
$$
第二个方程显然是原来的等式约束，但是第一个方程是什么？我们将其稍稍变形：
$$
\nabla f(x) = -\nabla \lambda h(x)
$$
这说明了什么？在驻点 $x$ 处，$f(x)$ 的梯度和 $h(x)$ 的梯度是平行的！我们使用直线约束来考虑一个简单的例子：
![[Pasted image 20230308083554.png|400]]
如图，黑色直线是约束条件，由于这是一条等高线，所以 $h(x)$ 的梯度必然和这条黑色直线平行，那么，当点 $x$ 在直线上移动时，如果 $f (x)$ 的梯度不是垂直于直线，那么 $f (x)$ 的函数值就会变化。$x$ 是驻点的情况就意味着 $f (x)$ 的梯度和直线垂直，也就是和 $h(x)$ 的梯度平行。
对于非线性的等式约束，原理都是一样的：
![[Pasted image 20230308084507.png|400]]

注意：乘子法算出的极值不一定是什么样子的，可能是最大/最小/鞍点，需要进一步判断！

### 含有不等式的情况，KKT 条件
这一节中，我们主要处理这样的问题：
$$
\begin{align*}
\min_{x}f(x)\\
s.t. \ \ \ g(x) &\le 0
\end{align*}
$$
那么，现在会有两种情况：最优值落在不等式约束的外部，那么约束中 $g (x) = 0$，如果最优值落在不等式约束的内部，那么约束中 $\lambda =0$，因此，无论是什么情况，总有 $\lambda g (x) = 0$
![[Pasted image 20230308085512.png|400]]

综上，我们构造的拉格朗日函数是：
$$
L(x,\lambda) = f(x)+\lambda g(x)
$$
极值点的位置满足：
$$
\begin{align*}
\nabla f(x)+\nabla g(x) &= 0\\
g(x) &\le 0\\
\lambda&\ge 0\\
\lambda g(x) &= 0
\end{align*}
$$
这就反映了要么 $g(x)=0$，要么 $\lambda=0$，总之 $\lambda g(x)=0$ 的情况。这一组条件合称 KKT 条件。

### 总结优化问题
最后，我们来写下多个约束条件同时存在时，KKT 条件是什么：
考虑优化问题：
$$
\begin{align*}
\arg \min_{x}f(x)\\
s.t. \ \ \ h_{i}(x) &= 0\\
g_{j}(x) &\le 0
\end{align*}
$$
构造的拉格朗日函数是：
$$
L(x,\lambda) = f(x)+\sum_{i} \lambda_{h,i}h_{i}(x)+\sum_{j}\lambda_{g,i}g_{i}(x)
$$
那么对应的 KKT 条件是：
$$
\begin{align*}
\nabla_{x,\lambda} &= 0 \\
h_{i}(x) &= 0\\
g_{j}(x) &\le 0 \\
\lambda_{g,j}&\ge 0 \\
\lambda_{g,j}g_{j}(x) &= 0 
\end{align*}
$$

## 优化视角下的矩阵分解
### 可以利用特征值分解求解的问题
#### 瑞丽商相关的优化问题
考虑下面的优化问题，其中，$A$ 是对称矩阵：
$$
\begin{align*}
\max_{v}v^{T}Av\\
s.t.\ \ \ v^{T}v &= 1
\end{align*}
$$
令：
$$
L(v,\lambda) = v^TAv-\lambda(v^Tv-1)
$$
两侧对向量 $v$ 偏导一次：
$$
2Av-2\lambda v = 0 \Rightarrow Av = \lambda v 
$$
使得这个优化问题取到极值的条件是：$v$ 必须是 $A$ 的特征向量，$\lambda$ 就是矩阵的特征值。那么
$$
v^{T} Av = v^{T}\lambda v = \lambda
$$
$v^{T}Av$ 的最大值就是矩阵的最大特征值，最小值同理，是矩阵的最小特征值。

与这个优化问题等价的一个问题就是瑞丽商的极值问题，也就是：
$$
\max \frac{x^{T}Ax}{x^{T}x}
$$
在上面，我们实际上是将 $x$ 的末端放在一个单位圆上进行讨论。这样做是可行的。因为分子和分母都是标量，如果 $x$ 的模变大，分子和分母也将同时扩大相同的倍数，因此这个式子的值应该只与 $x$ 的方向有关系。

再考虑一个优化问题（瑞丽商的一般问题）：
$$
\begin{align*}
\max v^{T}Av \\
s.t. \ \  \ v^{T}Bv  &= 1
\end{align*}
$$
同样的，只需构造
$$
L(v,\lambda = v^{T}Av - \lambda(v^{T}Bv-1)
$$
求一次偏导，得到：
$$
\frac{\partial L(v,\lambda)}{\partial v} = 2Av - 2\lambda Bv = 0 \Rightarrow Av = \lambda Bv
$$
也就是说，这个优化问题可以通过对矩阵 $B^{-1} A$ 的特征值分解来计算。

与之等效的优化问题显然是：
$$
\max_{v} \frac{x^{T}Ax}{x^{T}Bx} 
$$
其中，矩阵 $B$ 是对称矩阵，取 $x = B^{\frac{-1}{2}}y$ ，代入上式
$$
\frac{x^{T}Ax}{x^{T}Bx} = \frac{y^{T}B^{\frac{-1}{2}T}AB^{\frac{-1}{2}}y}{y^{T}y} 
$$
将 $B$ 写成特征值分解的形式：$B = V \Lambda V^{T}$ ，那么 $B^{\frac{-1}{2}}=V \Lambda^{\frac{-1}{2}} V^{T}$,==教材中声称将这个特征值分解的形式代入上式，就可以推导出一些结论，但是可能并不行== 

### 奇异值分解的优化视角
一个优化问题是这样的：
$$
\min ||Av|| \quad s.t.||v|| = 1
$$
这等价于以下问题：
$$
\min v^{T}A^{T}Av \quad  v^{T}v =1 
$$
又等价于以下问题：
$$
 \min (\frac{||Ax||}{||x||})^{2} = \frac{x^{T}A^{T}Ax}{x^{T}x} 
$$
这里对于 $A$ 是否为对称矩阵没有限制，因为 $A^TA$ 一定是实对称矩阵，可以被特征值分解

这样的优化问题与 SVD 有关系。我们知道，SVD（或者说 PCA）的思路都是试图将数据矩阵 $X$ 在一系列向量 $v$ 上投影，并且合理地选择 $v$ 的方向，企图让一系列的 $v$ 捕捉到数据中最有用的信息。
![[Pasted image 20230308101024.png|400]]

那么，我们知道，$v$ 可以转向不同的方向。如何评价 $v$ 是不是捕捉了数据中更多的信息呢？一种简单的想法是，当 $v$ 指向数据中信息最多的方向时，所有数据投影的和 $\sum y^{(i)}$ 会取到最大值（这可能和 PCA 的想法相违背，因为 PCA 是要提取方差最大的方向，后文中我们可能再对此做出讨论）。那么，如果我们将优化目标设置为最大化各个数据在给定方向上的投影的平方和，也就是最小化各个数据不在指定方向上的分量，那么目标函数：
$$
\max_{v}\sum_{i}y_{i}^{2} 
$$
或者
$$
\min_{v}\sum h_{i}^{2}
$$
显然，这个优化问题可以被改写为：
$$
\max \sum y_{i}^{2}  = ||y||_{L2}^{2}  = y^{T}y = (Xv)^{T}(Xv) = v^T X^{T}X v
$$
不失一般性，我们令数据向着单位向量上投影，那么
$$
v_{1} = \arg \max_{v} v^{T}X^{T}Xv \quad s.t. v^{T}v = 1
$$
那么我们在前面已经看到，这个式子最优化时，需要使得 $v$ 是 $X^{T}X$ 的特征向量！根据前面的对于这个最优化的推导，我们知道，最大特征值满足
$$
\lambda_{1} = v_{1}^{T}X^{T}Xv_{1} = ||Xv_{1}||^{2}= \sum y_{i}^{2} 
$$
在确定了 $X^TX$ 的第一个特征向量（这里也是 $X$ 的右奇异向量）之后，我们可以构造如下的优化问题，从而递推地求解出所有特征向量：
$$
v_{2} = \arg \max_{v} ||Xv|| \quad s.t. \ ||v|| =1 ,v \bot v_1
$$
### 中心化的数据
相信上面你已经发现了——使用 $\sum y_{i}^{2}$ 作为优化目标，真的能捕捉到数据中“变化最大”的方向吗？不太行！实际上，我们还差了一步——真正的 PCA 是在中心化后的数据矩阵上进行的。中心化意味着每个数据都要减去它的期望：
$$
X_{c}= X- E(X)
$$
记中心化后数据的投影为 $X_{c}v= y_c$ ，那么新的优化目标就是：
$$
v_{c,1} = \arg \max_{v} ||X_{c}v|| \quad s.t. ||v|| = 1
$$
现在，中心化后数据的 Gram 矩阵的最大特征值：
$$
\begin{align*}
\lambda_{1}\\
 &= s_1^2\\
 &= ||X_{c}v_{c,1} ||^2\\
&= \sum (y_{c,i})^{2}\\
&= (X_{c}v_{c,1})^{T} (X_{c}v_{c,1}) \\
 &= v_{c,1}^{T}X_{c}^{T}X_{c}v_{c,1} \\
&= (n-1) v_{c,1}^{T}\Sigma v_{c,1}
\end{align*}
$$
其优化目标就写成了找到数据方差最大的方向！但是，为何 $X_{c}^{T}X_{C}$ 与协方差有关，这留到后面再解释（你可能已经看出来了，$X_{c}^{T}X_{c}$ 度量的是两个特征之间的关联）。
![[Pasted image 20230308105927.png|400]]


## 补充小知识：矩阵的范数 (norm)
### P-Norm
P-范数的定义最为简单，即：
$$
||A||_{p}= \max_{x\not = 0 } \frac{||Ax||_{p}}{||x||_{p}}
$$
这可以理解为：矩阵代表的线性映射可以将入口空间中的单位向量最长能映射到多长？
那么，矩阵的 1-范数显然就是各个列向量的 1-范数的最大值；2-范数==需要证明==，是 $A$ 的最大奇异值；$\infty$ 范数也需要证明，是 $A$ 的行向量 1-范数的最大值。

### F-Norm
F-范数的定义更加简单，是所有元素的平方加和再开平方：
$$
||A||_{F} = \sqrt {\sum_{i}\sum_{j}a_{ij}^{2}}
$$
利用一个结论==需要证明== $\sum_{i}\sum_{j}a_{ij}^{2} = \mathrm{tr}(A^{T}A)$，那么
$$
||A||_{F} = \sqrt{\mathrm {tr}(A^{T}A)} = \sqrt{\sum \lambda_{i}}  
$$
显然可以看出，矩阵的 2-范数不大于 F-范数

## 从优化视角再看正交投影
我们这次一次性地观察数据往一组正交的向量上的投影
![[Pasted image 20230308130721.png|350]]

![[Pasted image 20230308130831.png|350]]

我们令 $y_1, y_2$ 分别代表所有数据在 $v_1, v_2$ 的方向上投影的坐标组成的列向量，那么，$y_{1}= Xv_1, y_{2}= Xv_{2}$ ，我们可以用以下几个量来度量 $y_1$ 和 $y_2$ 之间的关系：
$$
<y_{1},y_{2}> = y_{1}^{T}y_{2}
$$
$$
\cos (y_{1},y_{2}) =  \frac{y_{1}\cdot y_{2}}{||y_{1}||\ ||y_{2}||}
$$
由于 $y_{1},y_{2}$ 随着 $\theta$ 变化，因此，上面这些量都随着 $\theta$ 变化，进一步，将 $y_1 ,y_2$ 合并在一起，令 $Y = [y_{1},y_{2}]$，那么写出投影坐标的 Gram 矩阵
$$
G_{Y}= Y^{T}Y = (XV)^{T}(XV)  = V^{T}X^{T}XV = V^{T}G_{X}V
$$
把 $G_{Y}$ 打开，得到：
![[Pasted image 20230308131543.png|450]]
这个关于 $Y$ 的 Gram 矩阵集成了 $y$ 的模长和方向的信息，其中还包括了 $X$ 的 Gram 矩阵==虽然我们现在看来好像并没有什么用==

### 正交投影的统计视角
显然，原来的数据在每个特征上都有均值和方差，投影后的数据也有均值和方差。我们试图找出投影前、投影后的均值和方差的关系：
![[Pasted image 20230308132612.png|400]]
这主要有以下几个结论：
#### 均值
**投影的均值是均值的投影** 
$$
E(y_{i})= E(x)v_{i}
$$
#### 方差
实际上，协方差矩阵和惯量张量一样，都是一个张量——它们可以看作方向很难描述的向量，在空间中的不同取向都有不同的值==（注意，这里说的张量是物理学上的张量。和计算机科学中的定义可能存在偏差）== 想要将张量往一个方向投影，则：
$$
\mathrm{var}(y_{i}) = v_{i}^{T}\Sigma_{X}v_{i}
$$
而两个投影的协方差则是：
$$
\mathrm{cov }(y_{i},y_{j}) = v_{j}\Sigma_{X}v_{i}
$$
特别地，将 $y_i$ 作为列向量拼成矩阵时
![[Pasted image 20230308133411.png]]

#### 正交投影变换中的不变量
在正交投影的过程中，无论 $y_1, y_2$ 的方向如何改变, 总有一些量是不变的：

##### 各个投影向量二范数的平方之和不变！
这是因为
$$
\mathrm{tr}(G_{Y}) = \mathrm{tr} (V^{T}G_{X}V) = \mathrm{tr}(VV^{T}G_{X}) = \mathrm{tr} (G_{X})
$$
其中使用了矩阵迹的性质：
$$
\mathrm{tr}(AB) = \mathrm{tr} (BA)
$$

##### 所有投影向量解释的方差之和不变！
这显然可以利用
$$
\mathrm{tr}(\Sigma_{Y}) = \mathrm{tr}(\Sigma_{X})
$$
得到


在本章中，我们探讨了——拉氏乘子法的几何意义；KKT 条件
与特征值相关的几个优化问题（主要是瑞丽商），在优化视角下看奇异值分解，奇异值和投影之间的关系，对中心化后的数据进行的奇异值分解（这才是真正的 PCA）
矩阵的范数
投影前后统计量的关系，投影过程中的变量和守恒量（坐标向量模长之和、解释的方差）



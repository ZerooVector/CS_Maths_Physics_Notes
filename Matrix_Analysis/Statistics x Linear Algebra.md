#MA  
在这一节中，我们将使用线性代数的知识重构统计学中的一些基本内容。
![[Pasted image 20230311195550.png|300]]

## 均值
从样本中任意抽取一列数据 $x_{j}$，那么，这组数据的均值就是：
$$
E(x_{j}) = \frac{1}{ n} \sum_{i=1}^{n} x_{i,j}
$$
从线性代数的角度看，我们将全 1 的列向量记为 $\mathcal{1}$ ，那么，均值可以被写为：
$$
n\mu_{j} = \mathcal{1} x_j
$$
这样，就可以使得编写程序的时候更简洁（虽然对于我们，并没有什么用）
![[Pasted image 20230311195328.png|400]]

那么，我们来提供一个很新的视角：
$$
E(x)\mathcal{1 } = \frac{x_{j}^{T}\mathcal{1}}{\mathcal{1}^{T}\mathcal{1}} \mathcal{1} = \frac{x_{j}^{T}\mathcal{1}}{n}\mathcal{1}
$$
![[Pasted image 20230311200054.png|300]]

而剩下的部分则将在方差中讨论。显然，如果 $x$ 中的所有元素都相同，$x$ 就会与 $\mathcal{1}$ 共线。

## 质心（数据中心）
质心就是将数据的期望排成列向量：
$$
\mu_{X} = \begin{bmatrix}E(x_{1})  \\ E(x_{2})  \\ \cdots  \\ E(x_{D}) \end{bmatrix}
$$
对应的矩阵运算就是将每个维度上的数据都对着全 1 向量投影，再排列成列向量：
$$
\mu_{x}  = \frac{X^{T}\mathcal{1}}{n}
$$
为了区分，==在这里==我们==暂时==将期望向量定义为行向量
$$
E(X) = [E(x_{1}),E(x_{2}),E(x_{3}),\cdots ,E(x_{D})] = \frac{\mathcal{1}^{T}X}{n}
$$
## Z-score 标准化
我们首先进行中心化：首先，$\mathcal{1}^{T}x_{j}$ 取得了数据 $x_{j}$ 列的均值，然后，我们需要给他再乘一个 $\mathcal{1}$ 将这个均值“广播”出去，那么 $\mathcal{1}\mathcal{1}^{T}$ 是个矩阵，为了使得结果好看，我们给 $x$ 本身也配上一个矩阵，我们构造出一个“中心化矩阵”：
$$
M = (I - \frac{1}{n}\mathcal{1}\mathcal{1}^T )
$$
那么，这个矩阵的对角线上全都是 $1- \frac{1}{n}$，而非对角线全是 $-\frac{1}{n}$，容易发现这个矩阵起到中心化数据的作用：
$$
X_{c} = MX
$$
在实际编程的时候，我们会使用广播原则进行中心化：
$$
X_{c} = X - E(X)
$$
此外，$M$ 矩阵还有一个特性：是幂等矩阵，也就是说，$MM=M$
在中心化后，我们就可以进行标准化：
$$
Z_{X} = X_{c}S^{-1}  = (X-E(X))S^{-1}
$$
其中，$S = \mathrm{\sigma_{1},\sigma_{2},\cdots ,\sigma_{D}}$。

### 补充：数据的惯性
数据的惯性实际上是总离差平方和：
$$
SSD(X) = \sum_{i=1}^{n}\mathrm{dist}(x^{(i)},E(X))^{2}  = \sum_{i=1}^{n} ||x^{(i)} -E(X)||_{2}^{2} 
$$
就是每一条数据向量和均值向量之间的距离平方加起来，反映了数据偏离均值的程度。

我们可以给数据加上标签，计算类内质心、均值和方差
![[Pasted image 20230311204710.png|350]]

## 方差
概率论中定义的总体方差为：
$$
\mathrm{var}(x_{j}) = \frac{1}{n}\sum_{i=1}^{n}(x_{ij}-E(x_{j}))
$$
在统计学中，我们可以使用如下的样本方差作为总体方差的无偏估计：
$$
\mathrm{var}_e(x_{j}) = \frac{1}{n-1}\sum_{i=1}^{n}(x_{ij}-E(x_{j}))
$$
方差在实际操作中有多种写法，比如，利用广播或者中心化矩阵：
$$
\sum_{i=1}^{n}(x_{ij}-E(x_{j}))^{2} = ||x_{j}-E(x_{j})||_{2}^{2} = (Mx)^{T}(Mx) = x^{T}Mx
$$
从向量和投影的视角来看，之前我们想要求出数据的均值，那就是将数据向着一个全 1 的向量投影，然后将全 1 向量上的分量作为均值，而垂直于全 1 向量的部分则是方差：
$$
||x_{j}-E(x_{j})\mathcal{1}||_{2}^{2} = \sum_{i=1}^{n}(x_{ij}-E(x_{j}))^{2}
$$
![[Pasted image 20230311210233.png|300]]

## 协方差和相关系数
协方差被定义为：
$$
\mathrm{cov}(x,y) = \frac{\sum_{i=1}^{n}(x_{i}-E(x))(y_{i}-E(y_{i}))}{n} = \frac{(x-E(x)\mathcal{1})^T (y-E(y)\mathcal{1})}{n}
$$
协方差也可以使用中心化矩阵来写：
$$
\mathrm{cov}(x,y) = (Mx)^{T}My = x^{T}M^{T} My= x^T My
$$
Pearson 相关系数是标准化后的协方差：
$$
\mathrm{corr}(x,y) = \frac{\mathrm{cov}(x,y)}{\sigma_{x} \sigma_{y}} = \frac{(x-E(x))^{T}(y-E(y))^T }{||x-E(x)||\ \ ||y-E(y)||}
$$
回顾之前定义的余弦相似度：
$$
\mathrm{cos\ sim}(x,y) = \frac{x\cdot y}{||x||\ ||y||}
$$
可以看出，Pearson 相关系数就是标准化后的余弦相关系数

将数据矩阵的特征两两抽出，计算协方差，就可以构造出协方差矩阵：
$$
\Sigma = (\mathrm{cov}(i,j))
$$
那么，协方差显然可以用下面的方式进行计算：
$$
\Sigma = \frac{(X-E(X))^{T}(X-E(X))}{n-1} = \frac{X_{c}^T X_{c}}{n-1}
$$
我们发现——协方差阵和数据中心化后的 Gram 阵有关联！
这也回答了我们一直以来的疑问。奇异值分解首先需要找到最大的特征值->最大的特征值对应最大的奇异值->最大奇异值对应于协方差矩阵在某个向量上的投影最大。
协方差矩阵可以进行特征值分解，将其对数据的变换分解成旋转和缩放两部分：
$$
\Sigma=  V \Lambda V^{T} \Rightarrow  x^{T}\Sigma x = x^{T} V \Lambda V^{T} x =(V^{T}x)\Lambda(V^{T} x) = y^{T}\Lambda y
$$
其中，$y^{T} \Lambda y$ 就是一个正椭球了

与协方差矩阵类似，我们可以定义相关系数矩阵：
$$
P = \frac{((X-E(X))S^{-1})^{T}((X-E(X))S^{-1})}{n-1} = \frac{Z_{X}^{T}Z_{X}}{n-1}
$$
![[Pasted image 20230311212944.png|450]]



#MA 

#  正交投影：计量的开端

# 自行阅读部分

## 正交的定义，标量投影
如图，我们找到一个向量 $x$，让它在 $v$ 方向上投影
![[Pasted image 20230224073646.png|400]]

显然，在投影时，我们有 $(x-z) \cdot v = 0$ ，我们把其中一个向量转置一下，就可以写成矩阵乘法的形式：
$$
(x-z)^Tv = 0
$$
由于 $z$ 与 $v$ 的单位向量共线，因此，可以设：
$$
z = s \frac{v}{||v||}
$$
将一个式子代入另一个，显然有：
$$
(x - s\frac{v}{||v||})^Tv=0
$$
展开上面的方程，有：
$$
x^Tv - s\frac{v^Tv}{||v||} = 0 \quad \Rightarrow \quad x^Tv-s||v|| = 0
$$
那么，容易求出这个标量投影：
$$
s = \frac{x^Tv}{||v||} = \frac{x \cdot v}{||v||}
$$
另外，两个向量的夹角为：
$$
\cos \theta  = \frac{x^Tv}{||x||\ ||v||}
$$
## 向量投影
实际上，上面这个投影是有方向的（显然，就是 $v$ 的方向），那么我们给这个投影乘一个 $v$ 方向的向量，从而，
$$
proj(v)_x = s\frac{v}{||v||} = \frac{x \cdot v }{||v||}\cdot \frac{v}{||v||} = \frac{x\cdot v}{v\cdot v}v = \frac{x \cdot v}{||v||^2} v
$$
这里，$v$ 显然是决定方向的那一项，而前面的 $x\cdot v /||v||^2$ 则可以看成对向量 $v$ 的一个“缩放系数”
举个例子，我们应该如何导出二维平面上，向着方向向量为 $(\tau_1,\tau_2)$ 投影的变换矩阵？
我们令向量 $x$ 在 $\tau$ 上投影，那么：
$$
z = \frac{x\cdot \tau}{||\tau||^2} \tau = \frac{x_1\tau_1+x_2\tau_2}{||\tau||^2}[\tau_1,\tau_2] = \frac{1}{||\tau||^2}[(x_1\tau_1+x_2\tau_2)\tau_1,(x_1\tau_1+x_2\tau_2)\tau_2]^T
$$
上面的式子可以写成矩阵的形式予以化简，最终得到：
$$
z = \frac{1}{||\tau||^2}
\begin{bmatrix}
\tau_1^2 & \tau_1\tau_2\\
\tau_1\tau_2 & \tau_2^2
\end{bmatrix}
\begin{bmatrix}
x_1\\
x_2
\end{bmatrix}
$$
## 张量积的引入
接下来，我们引入一个在矩阵分析和量子力学中都十分重要的概念——张量积
给定两个矢量 $a$，$b$，我们知道，$a^Tb$ 得到的是它们的数量积，那么，现在我们引入一个全新的运算—— $ab^T$，这样，我们就会得到一个矩阵，这个矩阵称为两个向量的**张量积**
![[Pasted image 20230224081101.png]]

单单从运算的技巧上来看，张量积就是将两条轴上的两个向量张成了一张大网，像下面这样：
![[Pasted image 20230224082553.png]]

运算上，张量积带有一些性质，比如说，取对称以后运算顺序反向；分配律；结合律，等等
![[Pasted image 20230224083347.png]]
- 两个向量的张量积，秩自然是为 1 的
但是，实际上，张量积用来生成新的向量空间，至于为什么是这样，我们在后续讨论

## 向量投影-续
在前面的投影中，我们已经知道，如果 $v$ 是一个单位向量，那么 $x$ 在 $v$ 上的向量投影可以写成：
$$
proj_v(x) =(v^Tx)v = v(v^Tx) = vv^T x = (v\otimes v)x
$$
那么，$v\otimes v$ 中的两列显然就是两个基，与 $x$ 相乘得到新的向量。我们将这种——用于将一个向量变换到指定方向上的投影的矩阵，称为投影矩阵 (Projection Matrix)
利用张量积的写法，向着任意一个向量 $\tau$ 的投影矩阵可以被写为：
$$
X_{proj} = \left(\frac{\tau}{||\tau||}\otimes \frac{\tau}{||\tau||}\right)
$$

上文中的 $x$ 都是列向量，而在数据科学中，我们通常会使用一行来代表一个向量，那么，投影向量（矩阵）需要被乘在右边
![[ece4958b3138776a8f09974f288b32d.jpg]]

也就是，数据矩阵 $X$ 向着单位向量 $v$ 的投影是：
$$
Z= Xv
$$
数据矩阵的向量投影是
$$
Z = X(v\otimes v)
$$

## 正交矩阵的定义与性质
### 定义
前文中，我们介绍了向量向着向量的投影，在后文中，我们将来探究——向量能不能向着一个超平面投影呢？

### 对定义的第一种理解——列向量都是规范正交的
从定义上，正交矩阵是指矩阵的逆等于矩阵的转置，也就是：
$$
V^TV = I
$$
我们将这个展开写：
![[Pasted image 20230224091032.png|200]]
再展开：
![[Pasted image 20230224091053.png|400]]

那么，显然地，我们可以看出，$V$ 的列向量全部是单位向量，并且两两正交。此外，这样的矩阵称为 Gram 矩阵，这为我们后面批量计算向量的模和夹角提供了一种方式。具体的内容，我们将在后面详细介绍。

### 对定义的第二种理解——将 $VV^T$ 分解成一系列张量积
正交矩阵的定义式还可以反过来写：
$$
VV^T = I
$$
此时，将矩阵乘法展开写，我们将看到一系列张量积的和：
![[Pasted image 20230224091536.png|400]]

我们就可以发现，数据 $X$ 对着正交矩阵中的各个基做投影，最后得到的还会是数据本身
![[Pasted image 20230224091736.png|400]]

实际上，我们是对正交矩阵 $V$ 执行了分解，将其分成一系列张量积的形式。
![[Pasted image 20230224091908.png|400]]

### 正交变换的性质：保角、保长度、保面积
首先，正交变换下，长度不变！
$$
||V^Tx||_2^2 = (V^Tx)^T\cdot(V^Tx) =x^TVV^Tx = x^TIx = ||x||_2^2
$$
其次，正交变换下，角度不变。这是因为，我们可以将正交变换看作，仅仅是坐标轴都转过了某个角度。证明与上面是类似的。
最后，由于长度和角度均不变，所以正交变换前后，面积也是不变的！

## 利用投影看三个重要的运算
### 初等反射变换——Householder reflection
为了便于研究“镜像”的问题，我们首先只考虑发生在二维上的一些操作。假定我们的对称轴是 $\tau = [\cos \theta \ \sin \theta]^T$ ，我们先求向量 $x$ 在 $\tau$ 方向上的投影，也就是
$$
p = (\tau \otimes \tau)x
$$
![[Pasted image 20230224093543.png|350]]

从几何关系上，我们很容易看出，$x+z = 2p$
以上两式联立，整理得到：
$$
z = 2(\tau \otimes \tau)x-x = (2\tau \otimes \tau -I)x
$$
我们定义另外一个单位向量 $v$，使得 $v^T\tau=0$，则 $[v,\tau]$ 构成一组单位正交基。那么，它们分别构造的投影矩阵相加之后必然等于单位阵，也就是
$$
\tau \otimes \tau + v \otimes v = I
$$
以上联立，就得到了
$$
z = (I-2 v\otimes v)x
$$
令 $H = I-2v \otimes v$，那么，如果我们想要求得一个向量关于某一个超平面 $S$ 对称的向量，只需要选择这个超平面的一个法向量 $v$ 代入 $H$ 中，然后利用 $H$ 进行变换即可。$H$ 矩阵被称为 Householder matrix

### 构造正交的向量——Gram-Schmidt Orthogonalization
G-S 正交化实际上是一个递推地，不断构造与之前所有向量张成的超平面垂直的向量的过程。如果我们现在有 $D$ 个向量 $x_1, x_2,\cdots , x_D$，那么我们遵循如下的过程进行正交化：
#### 第一次
任意选择一个向量即可，$\eta_1 =x_1$
#### 第二次
![[Pasted image 20230224094547.png|300]]

我们一定要选一个与 $\eta_1$ 垂直的向量，一种好的做法是，任选向量 $x_2$，并在 $x_2$ 中，把它在 $x_1$ 上的投影减去。
$$
\eta_2 = x_2 - \frac{x_2^T\eta_1}{\eta_1^T\eta_1}\eta_1
$$
这里，$\eta_2$ 称为 $\eta_1$ 的正交补。
#### 第三次
第三次正交化中，我们选择的向量必须和 $\eta_1,\eta_2$ 张成的平面垂直，那么一种好的做法是任选向量 $x_3$，先减去它在 $\eta_1$ 上的分量，再减去它在 $x_2$ 上的分量。
![[Pasted image 20230224095002.png|400]]
以此类推，直到所有的正交基都被求解完成。

### 求解不相容的线性方程组——Ordinary Least Square
我们现在有线性方程组 $Xb = y$，其中，$X$ 的每一行代表一次观测数据中的解释变量（当然，$X$ 中有时会引入常数列“1”来为回归添加常数项，$X$ 中有时也会有傀儡（虚拟）变量），这个方程组的 $b$ 中仅仅有很少的参数，但是 $X$ 的行非常多，这使得方程组是无法求得解析解的，只能通过最小二乘法来求得近似解。
![[Pasted image 20230224235345.png|400]]
现在，$y$ 是一个 $n$ 维的向量，$X$ 是一个 $n$ 行，$m$ 列的矩阵。$n$ 是观测的次数，而 $m$ 则是回归参数的个数。我们希望，对每一组 $x_1,\cdots x_m$，都可以使用一组回归参数线性组合出一个 $y$。但是，这显然是不行的。由于参数仅仅有有限的 $m$ 个，所以 $X$ 的列向量张成的空间最高只是 $m$ 维的。我们只能将超高维（$n$ 维）的向量 $y$ 向着这个 $m$ 维的超平面投影，并将投影作为 $y$ 的估计值—— $\hat y$。剩下的向量 $y-\hat y$ 就是误差向量，我们希望误差向量最小，则其必须与 $X$ 的列向量（$m$ 个列向量就是超平面的基）张成的超平面垂直（这个超平面是 $n$ 维空间中的一个 $m$ 维平面，就像正方体的一个面一样）。因此，我们可以写出：
$$
X^T(y-Xb) = 0
$$
化简这个式子，得到：
$$
X^Ty-X^TXb = 0 \Rightarrow b = (X^TX)^{-1}X^Ty
$$

至此，我们研究了如何计算矩阵的投影，看到了矩阵投影在数据科学中的巨大威力。
现实中的数据通常是纷繁复杂的，例如——当我们分析一首音乐，音乐的响度、歌词、节奏，等等，都可能成为我们关注的指标，然而，我们不可能同时关注如此多的指标。在下一节中，我们将介绍如何利用正交分解的思想，提取数据中最重要的部分。

如果想要看更多线性回归的内容，请看计量经济学。
[[Multi Vars OLS]]

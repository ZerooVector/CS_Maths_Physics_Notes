#MA 

在接下来的几期中，我们将见到多姿多彩的矩阵分解
![[Pasted image 20230226204105.png]]

矩阵的分解类似于因式分解，是将一个矩阵分解为若干矩阵乘积的运算。从几何上，这些分解开的矩阵可能依次对应缩放、旋转、投影等运算。通过将叠加的矩阵分解为独立的个体，我们会更明晰地意识到这些矩阵的作用。

## Upper-Lower Decomposition
LU 分解将矩阵分解成一个上三角矩阵和一个下三角矩阵，可以视为 Gauss 消元法的矩阵乘法形式。
$$
A = LU
$$
有时候，我们也会执行 PLU 分解，例如 ```scipy.linalg.lu()``` 默认执行 PLU 分解：
$$
A  =PLU 
$$
这里 $P$ 是置换矩阵，它的任意一行或一列，它的作用是交换矩阵的各行（或各列）：
![[Pasted image 20230226204654.png]]

## Cholesky Decomp
Cholesky 分解是 LU 分解的特例。它将矩阵分解成一个下三角和它的转置的乘积。
$$
A = LL^T
$$
![[Pasted image 20230226205228.png|400]]

## LDL
LDL 又是 Cholesky 分解的一个特殊形式。
$$
A = LDL^T = LD^{1/2}(D^{1/2})^TL^T= LD^{1/2}(LD^{1/2})^T
$$
这里，$D$ 是一个对角阵，代表缩放变换，而 $L$ 是对角线全为 1 的下三角矩阵，对应一个剪切变换。因此，LDL 将矩阵分解成了剪切-缩放-剪切
![[Pasted image 20230226205759.png]]

## QR Decomp——G-S 正交化的矩阵形式
QR 分解将从原来矩阵不正交的列向量中抽取正交基和组合系数，通过正交基的线性组合就可以还原原矩阵。由于它可以看作对原矩阵列向量的 G-S 正交化，因此，任何形状的矩阵都可以 QR 分解。
设原矩阵 $X$ 的维度是 $n\times D$ ，那么 $x$ 的列向量是在 $\mathbb F^n$ 上的。在 QR 分解中，Q 矩阵的列向量是可以张成 $X$ 矩阵列向量的一组规范正交基，因此，$Q$ 矩阵也有 $n$ 列。在完全版的 QR 分解中，$Q$ 矩阵是 $n\times n$ 的方阵，为了和乘法运算的规则匹配，$R$ 矩阵是和 $X$ 矩阵大小相同的矩阵，每一列储存着 $X$ 的一个列向量在规范正交基下的坐标。QR 分解写为：
$$
X = QR
$$
![[Pasted image 20230226211034.png|400]]
$Q$ 矩阵的各个列互相正交，且都是单位向量，那么 $Q^TQ = I$。
注意到 $R$ 只有右上角部分是有数字的，因此，$R$ 完全可以切去一部分。$Q$ 中，右侧的几个基底也没有被使用，因此，$Q$ 也可以被切去一部分。在“切除”了“无用的部分”后，QR 分解可以被表示为：
$$
X = Q_{n\times D}R_{D\times D}
$$
![[Pasted image 20230226211807.png|400]]

此时，$Q$ 的各列依然正交，甚至 $Q^TQ=I$ 仍然满足。只不过，此时 $Q$ 不能再被叫做正交矩阵。因为正交矩阵的前提是方阵。
从图中还可看出，在生成 $X$ 的第一列时，实际上只使用了 $Q$ 的第一列，也就是说，$Q$ 的第一列和 $X$ 的第一列平行。$Q$ 的其他列，也是使用与 G-S 正交化相同的方式生成的（不断扣除在其他分量上的投影）。

## Eigen Value Decomposition
对于方阵 $A$，如果可以找到非零向量 $v$，使得：
$$
Av  =\lambda v
$$
则称 $v$ 是矩阵 $A$ 的单位向量。这个方程也可以写成
$$
(A-\lambda I) = 0
$$
我们使用二维矩阵来演示相似对角化的效果。设某个二维方阵 $A$ 有两个特征值、两个特征向量，则我们有：
$$
Av_1 = \lambda v_1 \qquad Av_2 = \lambda_2v_2
$$
我们试图把这两个式子写在一起，即：
$$
A[v_1 \ v_2] = [\lambda_1v_1\ \lambda_2v_2]
$$
为了使得形式上更好看，我们可以把 $\lambda_1,\lambda_2$ 再次提出到一个对角阵里面。最终，我们得到了：
$$
A[v_1\ v_2] = [v_1\ v_2]\mathrm{diag}(\lambda_1,\lambda_2)
$$
从而
$$
AV = V\Lambda \quad \Rightarrow \quad A = V\Lambda V^{-1}
$$
我们将 $p (\lambda) = \mathrm{det}(A-\lambda I)$ 称为 $A$ 的特征多项式，而 $p (\lambda) = 0$ 称为 $A$ 的特征方程。
由于特征值分解的意图无非是我们想要将 $A$ 的多个特征值放入一个方程。从方程的写法上，我们已经知道，$V$ 就是由特征向量作为列向量排列而成的，$\Lambda$ 就是用特征值作为对角元生成的矩阵。因此，我们可以手算特征值分解。

在对称矩阵的时候，EVD 出现特例——谱分解 (spectral Decomp)，也就是
$$
A =V\Lambda V^T
$$

## Singular Value Decomp
对于非方阵的 $X$ 来说，其自身显然不可能被特征值分解，但是，其 Gram 矩阵却可以进行分解。若 $X$ 矩阵是一个 $n\times D$ 的矩阵，那么 $X^TX$ 就是一个 $D\times D$ 的矩阵；而 $XX^T$ 就是一个 $n\times n$ 的矩阵。将它们分解完成后，得到的特征向量矩阵分别记为 $V$ 和 $U$。
奇异值分解如下：
$$
X_{n\times D} = USV^T
$$
![[Pasted image 20230226224316.png]]
那么，$S$ 是对角阵，对角线上的值被称为奇异值。而 $U$ 称为左奇异值向量，$V$ 称为右奇异值向量。$U$ 和 $V$ 都是正交矩阵，SVD 分解相当于同时完成两个 Gram 矩阵的奇异值分解。




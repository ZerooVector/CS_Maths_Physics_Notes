#MLorDS 
在 Gaussian NBC 中，我们将每个特征的条件边缘分布使用 Gaussian 分布估计，并且假设各个特征条件独立。现在，我们改变这个假设，转而假设各个特征的似然概率服从多元高斯分布。（不一定要求它们必须独立）：
$$
f_{X|Y} = \frac{\exp\left( -\frac{1}{2}(x-\mu_k)^T\Sigma_K^{-1}(x-\mu_k)\right)}{\sqrt{(2\pi)^D\Sigma_k}}
$$

## GDA 的基本思想
首先，NBC 考虑的是，将每一个点分入概率最大的类别中，也就是让每个点去找它最可能的那个分类。然而，GDA 的思想与其恰好相反——我们需要构造一个 Loss，这个 Loss 代表所有点被分错类的代价，之后最小化这个 Loss 即可。也就是：
$$
\hat y = \arg \min_{C_m} \sum_{k=1}^K f_{Y|X}(C_k|x)\cdot c(C_m|C_k)
$$
其中，$f_{Y|X}(C_k|x)$ 是一点 $x$ 被分到 $C_k$ 类的后验概率，$c(C_m|C_k)$ 是一个惩罚因子，如果在当前的判别中，$x$ 的正确分类和预测分类不同，则 $c$ 要取 1，否则取 0. 这个损失函数意味着，将“越可能正确的点”分错类，就要付出更大的代价。
这种模型的决策边界**有解析解**，并且，根据特征方差，可以分为 6 类，六类问题的决策边界的形状有明显的差距。
![[Pasted image 20230227083042.png]]
![[Pasted image 20230227083101.png]]
在 GDA 里面，区分决策边界是不是直线的重要标准是，各个变量的协方差矩阵是否相同。

## 求解决策边界
为了简单起见，我们仅仅使用两个特征的情况来讨论决策边界的形状。首先，为了去除 $\exp (\cdot)$，我们先定义某一类的判别函数是这一类的联合概率取对：
$$
g_k(x) =\ln (f_{X,Y}(x,C_k)) = -\frac{1}{2}(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k)-\frac{D}{2}\ln(2\pi)-\frac{1}{2}\ln|\Sigma_k|+\ln p_y(C_k)
$$
在两特征的情况下，决策边界显然应当是数据被分到两类的可能性相等的地方，也就是 $g_1 (x) = g_2(x)$，那么：
![[Pasted image 20230227084410.png]]
容易发现，这最大是个二次式！

## 两类判别分析
### 第一大类：线性判别分析 LDA
由于这个时候两个协方差矩阵相同，所以这时候，$x$ 的二次式被消去了。
![[Pasted image 20230227084635.png|450]]
其中，$x$ 前面的项就是决策边界（超平面）的法向量了！
![[Pasted image 20230227084801.png|400]]

### 第二大类：决策边界是二次（圆锥）曲线
![[Pasted image 20230227084927.png|450]]

## 从投影角度看线性判别分析
实际上，PCA 和 LDA 都是降维，只不过二者的操作方法不同——PCA 试图找出方差最大的几个相互正交的维度，从而在寥寥几次投影中储存数据中的大量信息；LDA 则希望找到可分性最好的投影方式：将所有数据打到一条直线上，要求类内间距尽可能小、类间间距尽可能大。
我们找到一条直线：
$$
w^Tx+b = 0
$$
先将均值向量向着直线的方向投影：
$$
\mu_{i,w} = w^T\mu_i
$$
再将协方差矩阵向着直线方向也投影：
$$
\sigma_{i,w}^2 = w^T\Sigma_i w
$$
##### 补充
怎么理解这个式子呢？我们知道，$\Sigma_i$ 右乘向量 $w$ 是行向量的投影、列向量的组合。你把第 $j$ 个方向上的数据投影，这个方向上均值的贡献就变成了原来的 $w_j$ 倍。于是，这个方向上所有方差的贡献就变成了原来的 $w_j^2$ 倍，所以有两个 $w$ 向量要被乘在式子里面。
==需要进一步的解释==
最终，我们的目标是：
$$
\arg \max \frac{w^T(\mu_1-\mu_2)(\mu_1-\mu-2)^Tw}{w^T(\Sigma_1+\Sigma_2)w}
$$





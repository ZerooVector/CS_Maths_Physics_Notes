#StochasticProcess 

采样有许多应用：
- 计算期望：$\mathbb{E} f (X) \approx \dfrac{1}{N} \sum_{i=1}^{N} f(X_{i})$
- 找到函数的极大或极小值：设 $U$ 是定义在 $\mathbb{R}^{d}$ 上的有下界的光滑函数，那么：$\dfrac{1}{Z_{\beta}}\exp ( - \beta U (x))$ 在 $\beta = T^{-1} \rightarrow \infty$ 时趋于 $\delta(x - x^{\star})$，其中 $x^{\star}$ 是 $U$ 的全局极小值。如果我们能够按照左侧的分布采样，我们就能在“低温极限”上找到 $U(x)$ 的极小值！这正式广为人知的“模拟退火”算法的基本思想
- 用于统计模型的参数估计。一个典型的例子是 EM 算法。
采样方法被广泛应用的两个领域是统计力学和统计学。在统计力学中，我们需要解决从一个 Gibbs 分布或者其他分布采样的问题，这是一个正问题；而在统计学中，我们要解决确定模型或者分布的参数的问题，这是一个反问题。我们将采样方法统称为蒙特卡洛方法。

我们首先考虑最简单的期望计算（数值积分）问题。设 $f$ 是 $[0,1]$ 上的连续函数，我们希望近似计算 $\int_{0}^{1} f(x)dx$。取 $\{X_{i}\}$ 是一列均匀分布在 $[0,1]$ 上的随机变量，那么：
$$
\int_{0}^{1} f(x)dx = \lim_{N \rightarrow \infty } \dfrac{1}{N} \sum_{i=1}^{N} f(X_{i})
$$
我们可以估计其误差，将误差定义为 $e_{N }= I_{N (f)}- I(f)$，那么：
$$
\mathbb{E}|e_{N} |^{2}= \dfrac{1}{N^{2}} \sum_{i,j = 1}^{N }\mathbb{E}[(f(X_{i})- I(f))(f(X_{j})  - I(f))] = \dfrac{1}{N} \mathrm{Var}(f)
$$
其中 $\mathrm{Var} (f) = \int_{0}^{1} (f(x) - I(f))^{2}$。那么我们就得到：
$$
I_{N (f)} - I(f) \approx  \dfrac{\sigma_{f}}{\sqrt N}
$$
对比梯形积分法得到的误差：
$$
e_{N} \approx \dfrac{1}{12} h^{2} \max_{x \in [0,1]} |f''(x)| 
$$
其中 $h$ 是划分区间的长度。我们可以大致认为 $h \approx N^{-1}$，那么在一维情况下梯形积分法的收敛速度占有明显优势。但是在 $d$ 维的情形下，梯形积分法的收敛速度只有 $h \sim \dfrac{1}{N^{\frac{1}{d}}}$，在四维以上，蒙特卡洛的收敛就会更快了。


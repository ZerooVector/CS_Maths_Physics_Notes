#StochasticProcess 

隐马尔可夫模型的假设是：我们有观测 $Y = (Y_{1},\cdots,Y_{N})$，以及一个马氏链的轨迹 $X= (X_{1},\cdots ,X_{N})$，这个马氏链的初始分布是 $\mu$，转移概率矩阵是 $P$。而观测值是由马氏链的状态和所谓发射矩阵 $R$ 决定的：
$$
r_{ij} = \mathbb{P}(Y = j |X = i)
$$
![[Pasted image 20240805095042.png]]

因此，一个 HMM 模型的参数是 $\theta = (P,R ,\mu)$。那么，在隐状态 $X$ 下出现观测 $Y$ 的概率是：
$$
\mathbb{P}(Y|X,\theta ) = \prod_{n=1}^{N} \mathbb{P}(Y_{n}|X_{n}  ,\theta)  = \prod r_{X_{n}Y_{n}}
$$
而在参数 $\theta$ 下产生隐状态序列 $X$ 的概率是：
$$
\mathbb{P}(X|\theta) = \mu_{X_{1}} \prod_{n=1}^{N-1} p_{X_{n}X_{n+1}}
$$
因此，联合概率是：
$$
\mathbb{P}(X,Y|\theta) = \mu_{X_{1}} r_{X_{1}Y_{1}} \prod_{n=1}^{N-1} (p_{X_{n}X_{n+1}}r_{X_{n+1} Y_{n+1}})
$$
HMM 模型一共有三种任务：
- 模型选择：给出多组 $\theta_{k}$，选出哪一组生成了真实的观测数据 $Y$
- 最优预测：给出观测序列 $Y$ 和模型的参数 $\theta$，得到最佳的隐状态序列 $X$
- 参数估计：给定观测序列 $Y$，估计参数 $\theta$，以最大化似然函数 $\mathbb{P}(X,Y|\theta  )$

我们先看模型选择任务：根据贝叶斯定理，我们显然有：
$$
\mathbb{P}(\theta_{k}|Y) \propto \mathbb{P}(Y|\theta_{k}) \cdot \mathbb{P}(\theta_{k})
$$
我们假设我们对于模型的选择没有任何的偏好，那么：$\mathbb{P}(\theta_{k}|Y) \propto \mathbb{P}(Y|\theta_{k})$，我们只需计算 $\mathbb{P}(Y|\theta_{k})$。我们使用前向算法计算这个概率值，定义：
$$
\alpha_{n}(i) = \mathbb{P}(Y_{1:n} = y_{1:n} , X_{n} = i |\theta)
$$
根据动态规划，显然有：
$$
\alpha_{n}(i) = \left(\sum_{j \in S} \alpha_{n-1}(j) p_{ji}\right) r_{iy_{n}}
$$

再看最优预测任务：我们有：
$$
\mathbb{P}(X|Y , \theta) = \dfrac{1}{\mathbb{P}(Y|\theta)} P(X,Y|\theta \propto \mathbb{P}(X,Y|\theta)
$$
因此我们只需最大化这个联合概率。这里设计的算法被称为维特比算法，定义：
$$
\delta_{n}(i) = \max_{x_{1:n-1}} \{\mathbb{P}(X_{1:n-1} = x_{1:n-1} ,X_{n}   = i , Y_{1:n} = y_{1:n} | \theta)\}
$$
那么可以写出递推式：
$$
\delta_{n+1} (j)  = (\max(\delta_{n}(j) p_{ij})) r_{jy_{n+1}}
$$

最后是参数优化：我们需要最大化 $\max_{\theta} \mathbb{P}(Y|\theta)$，定义：
$$
\beta_{n}(i) = \mathbb{P}(Y_{n+1:N} = y_{n+1:N } |X_{n} = i, \theta)
$$
那么 $\beta$ 的递推式是：
$$
\beta_{n}(i) = \sum_{j \in S} p_{ij}r_{jy_{n+1}} \beta_{n+1}(j)
$$
此外，我们定义：
$$
\xi_{n}(i,j) = \dfrac{\mathbb{P}(X_{n} = i,X_{n+1} = j, Y|\theta)}{\mathbb{P}(Y|\theta)} = \dfrac{\alpha_{n}(i) p_{ij}r_{jy_{n+1}}\beta_{n+1}(j)}{\sum_{i,j \in S}\alpha_{n}(i) p_{ij}r_{jy_{n+1}}\beta_{n+1}(j)}
$$
我们定义一个边缘：
$$
\gamma_{n}(i) = \mathbb{P}(X_{n} = i|Y,\theta) = \sum_{j} \xi (i,j)
$$
那么 $\sum_{n=1}^{N-1} \gamma_{n}(i)$ 就是在观测 $Y$ 下， 从状态 $i$ 出发的期望转移次数，而 $\sum_{n=1}^{N-1} \xi_{n}(i,j)$ 则是从状态 $i$ 转移到 $j$ 的期望次数。那么 Baum-Welch 算法实际上是一种 EM 算法，它采用如下的方式更新参数：
$$
\mu \leftarrow \gamma_{1}(i)
$$
物理意义：初始分布 $\mu$ 被更新为第一次从各个状态开始的概率
$$
p_{ij}\leftarrow  \dfrac{\sum_{i=1}^{N-1} \xi_{n}(i,j)}{\sum_{n=1}^{N-1} \gamma_{n}(i)}
$$
物理意义：$p_{ij}$ 是从 $i$ 转向 $j$ 的次数除以从 $i$ 转出的次数
$$
r_{jk} \leftarrow \dfrac{\sum_{n=1,Y_{n} = k}^{N} \gamma_{n}(j)}{ \sum_{n=1}^{N} \gamma_{n}(j)}
$$
物理意义：$r_{jk}$ 是隐状态处于 $j$ 时得到观测 $k$ 的次数除以隐状态处于 $j$ 的总次数。




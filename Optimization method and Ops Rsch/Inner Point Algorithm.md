#OPTandOR 

内点法的基本思想是：在可行域内按照梯度方向搜索最优解
![[Pasted image 20230309171213.png]]

显然，内点法的关键是防止搜索点跑到可行域之外。我们本课程介绍一种简单的方法：原始牛顿屏障法。
为了使用内点法，我们需要换一个线性规划的书写形式：
$$
\begin{align*}
\min \ \ c^Tx\\
s.t.\ \ Ax &\le b\\
var.\ \ x 
\end{align*}
$$
如何将等式约束升级到不等式约束？应当是将一个等式改写成两个不等式，两个不等式的等号都取到时，约束才被满足！

## 基本思路
内点法防止搜索点跑偏的基本方法是罚函数法（障碍函数法）。优化目标改写为：
$$
\min \ \ c^{T}x+\sum_{i=1}^{m}\mathbb{I}(\sum_{j=1}^{n}A_{ij}x_{j}-b_{i})
$$
其中
$$
\mathbb{I}(u) = \begin{cases}
0 \quad &u \le 0 \\  
\infty \quad &u > 0
\end{cases}
$$
然而，这样的障碍函数不是处处可微的。所以我们希望找到新的障碍函数：
$$
\mathbb{I}_{t}(u) = - \frac{1}{t} \log (-u)
$$
![[Pasted image 20230309173031.png|350]]

那么现在，对于每一个 $t$ 都可以求解到一个解 $x(t)^{\star}$ ，但是这个解是近似解。只有当 $t\rightarrow \infty$ 时，求出的近似解才逼近于原来的最优解。
![[Pasted image 20230309173602.png|300]]

因此，我们可以使得 $t$ 从 0 开始逐渐增大，从而使得最优点逐渐走向边缘。

## 优化问题的求解
我们只需要证明：
$$
g(x) = - \log (-\sum_{j=1}^{n}A_{ij}x_{j}+b_{i})
$$
是凸函数。为了证明，我们需要求解其一阶或者二阶导数：
![[Pasted image 20230316152800.png|500]]
Hessian 矩阵的分母都是相同的，因此，我们仅仅需要关注分子。我们发现，Hessian 矩阵可以写成梯度向量内积的形式。那么它显然是半正定的。
注意到：
$$
u^{T}\nabla g_{i}(x) (\nabla g_{i}(x))^{T}u=v^{T}v 
$$
这是一个内积的形式，因此，$u^{T}Hu \ge 0$ 自然成立。

**内点法（障碍函数法/罚函数法）为一系列的最优化问题都提供了很好的方法：讲约束条件塞入目标函数中，作为一种惩罚

![[Pasted image 20230316153903.png]]
我们将使得 $t$ 变化的迭代称为外迭代，而将牛顿法的迭代称为内迭代

![[Pasted image 20230316154126.png]]


![[Pasted image 20230316155043.png]]





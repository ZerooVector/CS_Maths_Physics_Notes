#RL 

## Dynamic Programming

### 策略迭代 (policy iteration)——累计平均，不断提升
强化学习中的 DP 由 $K$ 次策略的评估和 1 次策略的提升组成。
在每次策略评估中，要使用贝尔曼方程，计算每个动作的估价函数（或者每个状态的价值函数），之后进行策略提升

### 价值迭代 (value iteration)——贪心算法，单步最优
在策略迭代中，我们每次都会给出显式的最优策略（根据状态的价值函数大小给出最优策略！），但是价值迭代中不会给出，而是每次都使用**可以转移到的最优解**更新每个状态的价值函数
$$
V^\star (s) \leftarrow \max_a(R_s^a+\gamma\sum_{s'}P_{ss'}V^\star(s'))
$$

***然而，实际中，你根本不可能获知状态转移矩阵，因此，上面 DP 的思想是失效的***

## Monte-Carlo 方法
![[Ep.2 Some Algo to solve RL Problem 2023-02-27 19.16.10.excalidraw|500]]


### First-visit
利用每一回合中**第一次**访问到指定状态时，未来的收益，取平均值
$$
V(s) = \frac{G_{11}(s)+G_{21}(s)+\cdots}{N(s)}
$$
这种做法方差较小。

### Every-visit
利用每一回合中**每一次**访问到指定状态时，未来的收益，取平均值
$$
V(s) = \frac{G_{11}(s)+G_{12}(s)+\cdots+G_{21}(s)+\cdots}{N(s)}
$$

### 具体的做法
每次碰到这个状态，首先更新遇到该状态的次数，此后更新均值

优点：方法的误差与问题的维数无关，可以方便地解决统计意义的问题，无需对连续性的问题进行离散化
缺点：对于确定性问题，无法统计多条马氏链，需要转化成随机性问题；为了缩小误差需要很多计算步骤

### 探索与利用的权衡
在这个权衡中，最常用的探索策略是 $\epsilon-greedy$
我们选取小的概率 $\epsilon$ 来进行探索，其余的时候则贪心地选择策略

优点：
缺点：数据利用率低，需要无穷的时间

当然，也有其他策略，例如 Boltzmann 策略：
$$
\pi(a|s) = \frac{\exp(kQ(s,a))}{\sum_{a'} \exp(kQ(s,a'))}
$$

高斯策略：向动作中加入高斯噪声，也可以使得高斯噪声随着时间变小。

### 多摇臂老虎机 (MAB) 问题
有一排老虎机，但是每个老虎机的中奖率不同。如何通过尝试最大化收益？
$k$ 个动作的每一个都有一个期望收益，我们希望不断估计每个老虎机的期望收益，并且从中选择最大的。那么，我们会探索——估计收益；利用——选择目前期望最大的老虎机。

#### UCB 策略——选择可能事实上最优的动作

$$
A_t = \arg \max [Q_t(a)+c\sqrt{\frac{\ln t}{N_t(a)}}]
$$
我们选择：
- 收益高的
- 很长时间不选的

#### 随机梯度上升

## 时序差分算法（TD）
TD 解决了 MC 方法中，回合结束后才更新的问题，且需要大量的回合。因此，我们希望有一种单步更新的算法：
$$
V(s_t) \leftarrow V(s_t)+\alpha(R_{t+1}+\gamma V(S_{t+1})-V(S_t))
$$
实际上，$V$ 函数代表了对未来的估计
我们也可以使用 $n$ -step return ==回去需要再看看==

TD - 更新了某一个可能转移到的状态的价值函数
MC - 更新了一个回合中所有状态的价值函数
DP -更新所有相连的状态的价值函数

## Q-Learning
是一种 Model Free 的算法。我们建立一张 $Q$ 表，储存了状态 $S$ 下采取动作 $A$ 的估价
$$
Q(S,A) \leftarrow Q(S,A) + \alpha[R+\gamma \max_\alpha (Q(S',a)-Q(S,A))]
$$
$Q (S', A)$ 是 TD 方法里面新状态的估价函数。也就是说，我们不停地用本步的奖励，以及新状态的估价函数与旧状态之差来更新 $Q$ 表。
调参：
- $\alpha$：新知识有多大可能覆盖旧知识？$\alpha$ 可能是动态的、不断减小的！
- $\gamma$：
- $Q_0$ ：更高的初始值表示更乐观的估计

另外，一个变种是 Sarsa 方法：在更新时，仍然使用 $\epsilon-greedy$ 
$$
Q(S,A) \leftarrow Q(S,A) + \alpha[R+\gamma  (Q(S',a)-Q(S,A))]
$$
Q-Learning 在更新时，完全依赖于历史经验（off-policy, 行动时使用 $\epsilon-greedy$，评估时只是贪心策略）；Sarsa 仍然使用了一定的与环境的交互 (on-policy)。

##  Policy Gradient 
上面的方法难以处理高维度问题（尤其是无穷多动作的问题），因此，我们尝试直接迭代策略！
基于策略的方法，是参数化策略！$\pi_\theta (a|s) = P[a|s,\theta]$，这里的 $\theta$ 是一组数，甚至是神经网络的权值。实际上，就是把 $s$ 丢入函数，产生 $a$，我们需要调整函数的参数，使得输出最优！
### 总体思想


### 

## Actor-Critic
是一种将 Q-learning 和 Policy Gradient 结合的策略。






#RL 

## DDPG ——深度确定性梯度策略
这是一个 DQN 和 Actor-Critic 的结合体，一共有 4 个网络
主网络中包含 Actor Critic ，目标网络中也有 Actor Critic
总体来说，主网络中的 Actor 产生动作，Critic 评价主网络中 Actor 动作的好坏，目标网络负责评价主网络估价函数的好坏
在与环境交互时，模型加入了随着时间递减的高斯噪声，这是一个 trick，会使得模型探索的可能性增大

## TD 3——使用两套网络
- 目标网络使用两个 Critic 同时输出 $y$ 值，并且将最终的 $y$ 值取为二者最小值
- 为了防止进入劣化循环，先将价值估计的误差降低之后再进行策略更新（在实际算法中，模型直到一个 Episode 结束，才更新 policy network，而估价网络则是实时更新的）
- 在更新 $Q$ 的时候，在动作 $s'$ 上加一个小的扰动
$$
y = r + \gamma Q_{\theta'}(s' ,\pi_{\theta}(s')+\epsilon) \qquad \epsilon \sim \mathrm{clip}(N(0,\sigma),-c,c) 
$$
这样相当于增加了模型探索的可能性

上面，我们都是试图在 DQN 的基本框架下解决问题，但是，为了增强模型的探索性，我们可以使用多个 Actor

## A 3 C ——很多个 Actor 向主网络推送参数
- 首先，很多个子进程对环境的局部进行采样
- 采样完成后，子进程先更新自己的网络
- 子进程将自己的数据推送给主进程，主进程通过加权平均的方法更新网络参数
- 主进程将参数推给子进程
这其实是一个框架，适用于任何 A-C 的网络

## APEX——很多个 Actor 往经验池里面注入经验
这是一个与 A 3 C 类似的架构，但是子进程只进行交互，不会进行梯度下降。所有的经验被合起来放入同一个经验池，由主进程进行训练。每一条经验都有优先级
这只是一个框架，适用于任何网络

## IMPALA 

## PPO 
从 on policy 到 off policy ：
这个算法认为，考虑相对的奖励是重要的，因此，该策略给出一个“优势函数” $A$，这相当于一个学习率，控制了寻求极值的速度
另外，如何使得一个经验被多次利用？——重要性采样


->思考题： PPO 将 on 转 off ，然而，PPO 没有 Buffer，甚至是很小的 Buffer，为什么 PPO 的收敛速度比 DQN 更快？
提示：训练的过程
















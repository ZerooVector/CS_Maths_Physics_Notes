#RL 

## 多摇臂老虎机问题
UCB 为什么好？因为它做到了两件事：（1）收益低的动作以更低的概率被选择；（2）选择之前选择的次数更少的策略

梯度赌博机：从这里开始，算法转成 Policy-Based ，**策略**被显式地定义了。
给每个动作一个价值，并按照这个价值分配每个动作的概率。按照目前得到的价值与平均价值的差更新每个动作的价值函数

乐观初始值是什么？设置远高于实际收益期望的初始值，从而使得智能体做出决策后，状态（动作）估价函数会下降一段时间，从而强迫智能体对环境进行探索

## MDP 
贝尔曼方程：当前状态的价值 = 当前状态的奖励+后续状态价值依概率加权求和，(可以再对所有行动取加权平均)
这个理论上可以直接解出每个状态的价值函数，但是现实中肯定不可以这样算：求逆的复杂度太高，状态空间和动作空间都可能是无穷维的，实际的转移概率未知，这就是我们要使用 RL 的原因

- DP ：策略评估和策略提升
- MC：因为你不能准确地估计转移概率和回报，因此你可以尝试使用统计的方法获得转移概率和回报
- 


































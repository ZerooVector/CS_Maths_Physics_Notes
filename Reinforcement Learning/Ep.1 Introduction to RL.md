#RL 
Chiliu@bit.edu.cn
13718763223


## Introduction to RM
### RL 的特点
- 没有监督信号，只有奖励反馈
- 反馈具有延迟性
- 动作会影响后续数据
- 是一个与时间序列相关的序贯决策过程

### 算法分类
- Model Free：状态转移概率未知 / Model Based ： 状态转移概率已知，在现实中，由于状态太多了，通常是 Model Free
- Policy-Based: 输出动作的概率分布 / Value-Based：输出未来的期望价值
- MC-update ：游戏回合结束才更新 / Temporal-Diffence update：单步更新
- On policy : 同步探索环境和更新策略 / Off policy ：探索环境和更新策略是异步的，今日的前沿是On Policy

### RL 方法的三个部分
- State : 尽可能准确地描述外界的环境，尽可能地包含充足的信息（尤其是时序信息），
- Action : 离散/连续
- Reward : 仅仅代表 Agent 在当前时刻的表现，Agent 的任务是最大化累计奖励。

- **Reward Hypothesis**： 所有的任务目标都可以用最大化期望奖励描述
- 需要注意的是，某些任务的奖励是稀疏的

由 $s, a, r$ 组成的序列称为 History:
$$
H_t= s_1,a_1,r_1,...,s_t,a_t,r_t
$$
于是，
$$
a_{t+1} = f(H(t))
$$

## 马尔可夫性和马尔可夫过程
马尔可夫性假设说明，下一时刻的状态仅仅与目前状态有关（历史都是可以抛却的）：
$$
P(s_{t+1}|s_{t}) = P(s_{t+1}|s_1,\cdots s_n)
$$
马尔可夫过程要求一个有限状态集合 $S$，和转移概率矩阵 $P$，其中，$P_{ij}$ 代表从状态 $i$ 转向 $j$ 的概率

### 马尔可夫奖励过程
在一个马尔可夫奖励过程中，我们当前这一时刻给出的奖励，等于下一时刻奖励的期望。也就是
$$
R_t = E[R_{t+1}|S_t=S] \qquad (\star)
$$
由于我们强化学习的目标是要最大化期望累计奖励，但是，太远期的奖励我们猜不准，于是，我们通常会将远期的奖励乘以折现因子，也就是
$$
G_t = \sum_{i=t+1}^\infty R_i\gamma^{i-1}
$$
来作为我们的累计回报。

#### 对状态进行估价
利用一个状态未来可能得到的奖励的多少，我们可以对这个状态进行估价。估价函数就是从当前状态开始，采取策略 $\pi$，未来收益的期望，也就是：
$$
\begin{aligned}
V_\pi(s) &= E_\pi[G_t|s_t=s]\\
& = E_\pi[R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\cdots+\gamma^{n-1}R_{t+n}|s_t=s]\\
& = E_\pi[R_{t+1}+\gamma (R_{t+2}+\gamma R_{t+3}+\cdots+\gamma^{n-2}R_{t+n})|s_t=s]\\
& = E_\pi[R_{t+1}+\gamma G_{t+1}|s_t=s]\\
& = E_\pi[R_{t+1}+V_\pi (s_{t+1})|s_t=s]
\end{aligned}
$$
这样，我们就把奖励写成了一个递归的形式，利用 $\star$ 式，在这一步我们给出的奖励，就是 $t+1$ 时刻所获得奖励的期望，那么我们就可以把上式拆成此步奖励和下步的期望奖励：
$$
V(s) = R_s+\gamma \sum_{\forall\ possible\  s_{t+1}} P_{s_t\rightarrow s_{t+1}}V(s_{t+1})
$$
然后这样我们发现，问题化成了一个巨大的线性方程组（至少可以求解），但是，想要求解仍然需要 $O(n^3)$ 的时间，现实中不可用。


## 马尔可夫决策过程
现在，我们将动作加入我们的考虑。我们记一个人目前位于状态 $s$，采取动作 $a$，转移到状态 $s'$ 的概率为 $P_{ss'}^a$，也就是：
$$
P_{ss'}^a = P[s_{t+1}=s'|s_t=s,A=a]
$$
我们的智能体可以有一些策略，这些策略可能是不确定性的，即针对目前状态 $s$，按照一定概率选取数个策略：
$$
\pi(a|s) = P[A_t=a|S_t=s]
$$
或者是确定性策略
$$
a=\pi(s)
$$
#### 状态和行动的估价
在引入了策略之后，我们仍然可以对目前的状态，以及目前状态下采取的行动进行估价：
现在，我们的“轨迹”是看到状态->做出行动->达到新的状态->再做出行动，循环往复。
![[Ep.1 Introduction to RL 2023-02-27 17.58.18.excalidraw]]

一个行动的估价函数被定义为采取这个行动自身的回报加上这个行动可达状态的期望回报，也就是采取这个行动之后的总回报：
$$
q_\pi (s,a) = R_s^a+\gamma\sum_{\forall \ possible\ s'\ via\ a}P_{s\rightarrow s'}^av_\pi (s')
$$
而一个状态的估价函数则被定义为这个状态的期望收益：
$$
v_\pi(s) = \sum_{a \in A}\pi(a|s)q_\pi(a|s)
$$
不难发现，这两个式子可以互相带入。将 $q$ 代入 $v$，那么：
$$
v_\pi(s) =\sum_a \pi(a|s)[ R_s^a +\gamma \sum_{\forall \ possible\ s'} P_{s\rightarrow s'}v_\pi (s')]
$$
$R_s^a$ 是目前状态采取 $a$ 的收益，$v_\pi$ 是下面的点的状态估价函数。
将 $v$ 代入 $q$，那么：
$$
q_\pi(s,a) = R_s^a+\gamma \sum_{s'}P_{ss'}^a\sum_{a}\pi(a'|s')q_\pi(a'|s')
$$
一个最优的策略，就是当前状态的状态价值函数最大化！





































#RL 

在本节中，我们将讨论一些多元函数，这些函数定义在 $\mathbb{R}^{n}$ 上，值域在 $\mathbb{R}$ 中，我们将梯度 $\frac{\partial f}{\partial x}$ 看作一个行向量，这样，我们就有：
$$
f(x + \Delta x) \approx f(x) + \frac{\partial f}{\partial x} \Delta x 
$$
我们再考虑一个向量值函数 $g (y):\mathbb{R}^{m} \rightarrow \mathbb{R}^{n}$，那么它的梯度必须是 $n \times m$ 的矩阵。
在这种向量值函数中，链式法则仍然成立：
$$
f(g(y + \Delta y)) = f(g(y)) + \frac{\partial f}{\partial x} \frac{\partial g}{\partial y} \Delta y 
$$
特别地，我们暂时将 $\nabla f(x)$ 定义为一个列向量，定义：
$$
\nabla ^{2} f(x) = \frac{\partial }{\partial x}(\nabla f(x))
$$
我们接下来想要完成一些任务。

## 方程求根

我们将经常遇到方程求根问题。例如，求解连续时间系统的平衡点时，我们考虑方程：
$$
f(x^{\star}) = 0
$$
在求解离散时间系统的平衡点时，我们考虑方程：
$$
f(x^{\star}) = x^{\star} 
$$
（显然，如果一个平衡点是稳定的，那么找到它的最简单的方法是不断运行系统，直到系统自己达到稳定平衡点。**注意：如果你使用前向欧拉法，这就是计算机科学中的梯度下降**）

### 牛顿方法
牛顿方法是这样求解方程的：
- 在当前的位置找到方程组的线性近似
- 求解线性方程组
考虑到近似后的方程组为：
$$
f(x) + \frac{\partial f}{\partial x}\Delta x  = 0 \Rightarrow \Delta x  = (\frac{\partial f}{\partial x})^{-1}f(x)
$$
那么我们得到新一次的 $x$：
$$
x \leftarrow  x + \Delta x 
$$
不断重复这个过程，直到收敛
这些方法可以使得我们求解隐式格式。其中，最慢的部分是求解线性方程组。对于稀疏的雅可比矩阵，我们可能找到更简单的方法

## 最优化
显然，对于一个多元函数，如果它是连续的、可导的，最小化的必要条件是：
$$
\frac{\partial f}{\partial x} = 0 
$$
那么，我们就把一个最优化问题转换成了一个求根问题，这可以使用牛顿法解决。
$$
\nabla f(x+ \Delta x) = \nabla f(x) + \frac{\partial }{\partial x} (\nabla  f(x)) \Delta x = 0 \Rightarrow \Delta x = - (\nabla^{2} f(x) )^{-1} \nabla  f(x)
$$
实际上，使用牛顿法求解最优化问题还有另一个解释：我们在局部使用一个二次函数来近似原函数，然后我们试图最小化这个二次函数。因此牛顿法是一个二阶方法。
**但是，由于牛顿法是在求解导数的零点，因此它不一定找到全局极大/小值，你甚至都不知道它找到了极大/小值**

### 充分条件
显然，只使用必要条件必然不行，因此我们需要找到最优化的充分条件.
我们将 $\nabla^{2}f$ 称为 Hessian 矩阵。如果 Hessian 矩阵在任何位置都是正定的，那么我们称 $f(x)$ 是**strongly convex**，这样的问题可以使用梯度下降或者牛顿方法求解。但是，对于非线性的问题，这通常不成立
那么，我们可以有一个方法：每次执行牛顿法时，我们都检查 $H$ 是否正定。如果不正定，我们使得：
$$
H = H + \beta I
$$
这里的 $\beta$ 是一个超参数。这样我们可以使得负的本征值变成正的。我们把这种手段也称作 Regularization（正则化），或者称为 damped Newton（阻尼牛顿法）
为什么 Hessian 矩阵正定就会得到好的效果？显然，你可以发现，对于一元函数的情况，如果 Hessian 矩阵为负，那么你将会得到梯度上升。此外，这种方法只能解决凸优化问题，Hessian 矩阵为正代表着问题的凸性。




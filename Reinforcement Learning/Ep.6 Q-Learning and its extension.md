#RL 

从上面的 TD 估计，我们就可以导出 Q-Learning 算法。该算法中最重要的部分是对 Q 值的更新。我们记 $(s,a)$ 二元组历经了 $n(s,a)$ 次，那么：
$$
\alpha  = 1/n(s,a)
$$
更新方式：
$$
Q(s,a) \leftarrow  Q(s,a) + \alpha (r + \gamma V(s')  - Q(s,a)) \quad  V(s') = \max_{a'} Q(s',a')
$$

在强化学习中，一个重要的问题是**探索与利用**的权衡。显然，如果我们使用纯随机策略，那么随着智能体不断与环境交互，智能体是有一定概率学到最优策略的，但是，这样做的效率太低。我们有几种做法来实现这个 E-E 平衡：
- Epsilon-greedy：小概率执行随机策略
- Boltzmann Exploration：
$$
P(a) = \dfrac{\exp(\frac{Q(s,a)}{T})}{Z}
$$
然后，我们还会有一种算法叫做 Sarsa 算法。它稍微改变了对 Q 表的更新方式，使用 Epsilon-Greedy 更新 Q 表。Sarsa 是一种 On-Policy 算法，而 Q-Learning 是一种 Off-Policy 算法。Sarsa 的最大作用是阻止对 Q 值的过高估计。

Q-Learning 算法的收敛需要如下的（直观上的条件）：
- 每一个状态都被无限次数地访问到
- 随着时间越来越长，动作的选择变得越来越“贪心”（？）


但是，表格型 Q 学习无法求解更加复杂的问题。例如高维的、连续的状态空间和动作空间。我们希望找到一个函数，来近似地表达这个 Q 表。显然，一种简单的方式是：
$$
\text{Err}(w) =  \frac{1}{2} [Q(s,a;w) - r - V(s')]^{2} \quad  V(s') = \max_{a'} Q(s',a';[w])
$$
如果 $Q(s,a;w)$ 是线性函数，那么这个算法可以收敛（？），但是，很多时候，使用非线性函数的时候，这些算法不能收敛。我们有一些技巧：
- Experience Replay：收集 $(s,a,s',r)$ 存入缓冲区，每次从缓冲区中采样一个 Batch。作用：重复利用、打破了动作间的连续性（这使得我们可以一次性调整整个 $Q$ 曲面，而不只是在一个小区域中调整）、数据利用率升高、
- Target Network：专门使用一个网络来估计 Q 值
- 











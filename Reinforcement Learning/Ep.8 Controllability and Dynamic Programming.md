#RL 

## Infinite Horizon LQR
在我们期望的控制时间无限长的时候，对于时不变的 LQR，$K$ 矩阵将趋近于某个常值（从实验中，我们可以看出这一点，在这里，我们略去证明）。因此，我们有时直接使用这一常值代替变化的 $K$（尤其是在一类被称为 Stablization Problems 的问题中，这类问题包括小车的轨迹跟踪、无人机控制，等等）。回忆 Ricatti Recursion：
$$
K_{i} = (R + B^{T}P_{i+1} B)^{-1} B^{T} P_{i+1} A
$$
$$
P_{i} = Q + A^{T} P_{i+1} (A - BK_{i})
$$
我们可以使用牛顿法求根来找到 $K$ 和 $P$ 的极限值
>[!info]
>在 Julia/Matlab/Python 中都有执行 Ricatti 递归的工具箱
>

## 能控性判据
对于一个初态 $x_{0}$，我们尝试计算 $x_{N}$，利用系统动力学方程进行递推，容易得到：
$$
x_{N} = A^{N}x_{0}+ A^{N-1} B u_{0} + A^{N-2} Bu_{1} + \cdots 
$$
那么，将上式写成矩阵方程：
$$
x_{N} = \begin{bmatrix}B & AB & A^{2}B  & \cdots  & A^{N-1}B \end{bmatrix} \begin{bmatrix}u_{N-1} \\  u_{N-2} \\ u_{N-3} \\ \cdots  \\ u_{0}\end{bmatrix} + A^{N}x_{0}
$$
由于该系统是线性的，不失一般性，我们尝试将系统转移到 $x_{N} = \vec 0$（其余情况均可通过仿射变换得到）。这个方程的未知数维度相当大，但方程的数量却相当少（状态空间有几个维度，方程就有几个）
>[!warning]
>课程中介绍这是一个最小二乘问题，但是，本问题中未知数的数量看起来大于方程数量，因此这应该是一个超静定问题。对于超静定问题，使用伪逆求解时，我们可以得到最小范数解。

我们得到的解是：
$$
\begin{bmatrix}u_{N-1}  \\ \cdots  \\ u_{0}\end{bmatrix} = [C^{T}(CC^{T})^{-1}] (x_{N} - A^{N}x_{0})
$$
其中：
$$
C = \begin{bmatrix} B & AB & \cdots & A^{N-1}B\end{bmatrix}
$$
注意到，矩阵 $C$ 的秩最大为 $n = \mathrm{dim}(x)$，那么要想使得上式中的 $CC^T$ 可逆，矩阵 $C$ 的秩必须为 $n$ 。这就是系统能控性的秩判据。
根据 Cayley-Hamilton 定理，我们可以证明 $A^{N}$ 可以被写作：
$$
A^{N} = \sum \alpha_{i} A^{i}
$$
因此，后面的很多“块”都可以写成前面的“块”的线性组合，后面的这些“块”就不会改变矩阵 $C$ 的秩。那么，我们可以将秩判据简化为计算
$$
C = \begin{bmatrix} B & AB & \cdots & A^{n-1}B\end{bmatrix}
$$
的秩。我们可以不加证明地将这一判据推广至连续时间的情况。这里的 $C$ 被称为“能控性矩阵”。对于时变系统，仿照前面的例子进行推导即可。

## Bellman's Principle
我们注意到：
- 最优控制问题通常表现出重复的序贯结构
- 这种系统通常是因果系统，过去的控制只会影响未来

那么，我们给出贝尔曼原理（最优性原理，The Principle of Optimality）：
**Sub-trajectories of optimal trajectories have to be optimal for appropriately defined sub problems.**

## 动态规划
那么，根据贝尔曼原理，我们可以从轨迹的末端开始，不断地解决一个一个子问题，并最终得到最优轨迹
我们定义：从状态 $x$ 开始执行最优控制，直到末态，这一过程中的成本记为 $V(x)$。
我们先在 LQR 上演示动态规划。显然：
$$
V_{N}(x) = \frac{1}{2} x^{T} Q_{N} x  = \frac{1}{2} x^{T} P_{N} x
$$
我们向前一步：
$$
\begin{align*}
V_{N-1}&= \min_{u} \frac{1}{2} x_{N-1}^{T} Q x_{N-1} + \frac{1}{2} u^{T} Ru + V_{N} (A_{N-1} x_{N-1}+ B_{N-1} u)\\
&=  \min_{u} \frac{1}{2} u^{T} Ru + (Ax_{N-1} + Bu)^{T} P_{N}(Ax_{N-1} + Bu)
\end{align*}
$$
通过求梯度，我们可以得到：
$$
u_{N-1} = - (R_{N-1} +B_{N-1}^{T} P_{N} B_{N-1}) B_{N-1} P_{N}A_{N-1}x_{N-1}
$$
我们简记为 $u_{N-1} = - K_{N-1} x_{N-1}$，将这个方程代入上式，从而消去 $u$：
$$
V_{N-1}(x) = \frac{1}{2} x^{T}[Q + K^{T} RK + (A-BK)^{T} P_{N}(A-BK)]x
$$
简记为 $V_{N-1}(x) = \frac{1}{2}x^{T}P_{N-1}x$
那么我们就可以得到一个递推方程，递推出 $P$ 和 $K$。这仍然是 Ricatti 递归。

让我们给出动态规划的一般定义：边界条件：
$$
V_{N}(x) = l_{N}(x)
$$
递推过程：
$$
V_{i-1} (x) = \min_{u}[l(x,u) + V_{i} (f(x,u))]
$$
也就是本步的损失+之后的最小损失。同时，我们也得到了最优策略序列。
我们也可以使用动作价值函数 ($Q$ 值)写出动态规划方程：
$$
Q_{i}(x,u) = r(x,u) + V_{i+1}(f(x,u))
$$
我们可以使用神经网络来估计 $V$ 函数，从而我们不需要系统的完整数学模型

那么，DP 已经可以帮助我们求得这些序贯决策问题的最优解，但是，我们只能对于 LQR 这样的简单问题得到最优解，但是我们无法求解相当复杂的问题（无法对一般的优化问题写下其解析形式，因为你在每一步回溯中都需要求一次最小化）。因此，对于 $V$ 或者 $Q$ 的估计是动态规划中的重要问题，这也是现代强化学习算法的基础！



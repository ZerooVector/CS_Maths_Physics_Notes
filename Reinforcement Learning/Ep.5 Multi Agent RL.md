#RL 

- 在多智能体的强化学习中，可能出现智能体之间的合作、竞争和混合类型。合作和竞争是很好理解的，直观上来说，按照最小最大化原则处理对策矩阵，那么纯策略 Nash 均衡会落到不同的位置。实际上，多智能体的学习目的也是使得多智能体收敛到 Nash 均衡
- 在混和类型中，多个智能体可能处理一个大型的任务, 它们会竞争地处理任务中的不同部分，但是合作将带来最佳收益
- 在多智能体强化学习中，每个智能体并不能看到全局信息，而只能看到自己眼前的一部分
- 多智能体系统的核心是将系统分成若干个智能的、自治的子系统，多个智能体需要相互通信，相互协调。一般来说，多智能体系统的通信开销非常大，因此，一种研究方向是推断在合适的时机将合适的信息与合适的个体通信
- 目前，多智能体的研究方向包括：
	- 稳定性：系统如何收敛到 Nash 均衡
	- 适应性：当他人的策略略微改变时，系统的表现不会太差
	- 通信问题
- 此外，现在研究的都是 CTDE 问题（中心化训练，去中心化执行）；真实的难题是 DTDE 问题

## POMDP（部分可观测的...）
使用一个七元组定义 POMDP: $(S, A, T, R, O, Z,\gamma)$ ，其中，$O$ 被称为“观察结果集”，而 $Z$ 定义为：
$$
Z(s',a,o') = P_{r}(O^{t+1} = o|S^{t+1} = s',A^{t}= a)
$$
因此，在环境不能完全可观时，智能体仅仅能获取当前身处各个状态的概率（你可以使用现代控制里面的“能观性”理解）

## 常用的算法：基于值函数的
- DQN 的扩展：认为每个智能体可以观察到全局状态 $s_t$ ，选择单独的动作 $a_{t}$，并获得可以共享的团队奖励 $r_{t}$，每个智能体的 $Q$ 函数各自学习，但是使用的信息是全局的状态和自己的动作（注意：这个东西不能将自己的动作对环境的影响分离开来）。这个算法可以学习简单的问题，但是不能处理大型问题和环境非平稳问题。
- RIAL：此时让人们意识到每个智能体不可能观察到全部的环境。这个时候，每个智能体会获得自己的观测值、上一步发状态、以及其他智能体传递过来的信息。在这种情况下，使用经验池会使得训练更差，因为之前的经验和现在的经验是从不同的分布中采样的。
- VDN：认为整个系统的 Q 函数是所有智能体 Q 函数的和，这样的话，要求整体的 Reward 是部分 Reward 之和，但是这是不成立的。
- QMIX：是对 VDN 的扩展，它认为，总体的 $Q$ 函数不能直接使用求和表达，而应该使用 $$
\frac{\partial Q_{tot}}{\partial Q_{i}} \ge 0
$$ 来表达这样的一种关系。因此，QMIX 使用一个网络来聚合所有的 Q 函数。该算法在 StarCraft 上取得了很好成绩

## 基于策略的
- MADDPG：使用一个简单的假设：状态转移只与动作有关，而与策略无关





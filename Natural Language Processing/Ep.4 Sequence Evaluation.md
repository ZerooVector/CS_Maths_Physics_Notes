#NLP 

语言模型只做一件事：给出每个句子出现的“可能性”。语言模型经常作为“先验”，出现在其他模型中。
##### Finite State Machine 
只能表示相当简单的语言，例如现在的正则表达式


## 基于统计的语言模型
我们现在希望知道，如何求出一个句子出现的概率。我们希望对：
$$
P(\vec w)
$$
进行分解。一种最简单的分解显然是：
$$
P(\vec w) = \prod_{i}P(w_{i})
$$
这个假设太强了，我们通常使用条件独立性假设：
$$
P(U) = \prod_{i} P(u_{i}|context_{i})
$$
$context_{i}$ 是上下文。只使用上文时，我们称为 AUTO Regression；使用上下文时，我们称为 AUTO Encoding 。那么，这种语言模型的训练目标是最大化后验概率，在解码过程中，我们也按照后验概率选择输出。

### N-Gram Model
由于纯使用自回归会导致参数规模过大、各项概率太小，**某个词出现的概率仅仅与前面的 $n$ 个词有关**。事实证明，这种简单的模型比带有规则模型更好。**(容易出题)** 
##### 例子：N-gram 汉语分词
我们将一个句子所有的可能性使用一张图来建模，使用动态规划求解。
![[Pasted image 20231012144721.png|450]]


## 基于神经网络的语言模型
### NNLM
最早使用神经网络来做语言模型，实际上是使用逻辑回归来做的。
![[Pasted image 20231016153947.png|450]]
在这里，每个词不再以 One-Hot 编码，而是使用一个稠密向量表示。一个新的纪元开始了。而且，这个模型的优势是直观上考虑了线性和非线性的特征。这样，我们发现，如果某两个词前面的几个词是一样的，那么这两个词的词向量非常接近。因此，矩阵编码了词语间的**共现特征**（注意，这仍然是一个 N-Gram 模型）。这个模型的问题是：窗口的长度是固定的，无法捕捉句子中的长距离依赖。
使用 RNN 可以解决这个问题，同时可以解决变长序列问题。在这种情况下，两个上文类似的词将获得类似的向量表征


### CBOW 
![[Pasted image 20231016160850.png|450]]
通过内积衡量了词语间的相似度。
>[!info]
>这里的线性模型赋予了模型通过矢量运算进行类比任务的能力吗？

如何使用词向量？有一些逆天模型
![[Pasted image 20231016162550.png|400]]

![[Pasted image 20231016164312.png|400]]

>[!note]
>语义空间？情感空间？注意表示学习的重要性！

![[Pasted image 20231016164636.png|450]]
这是一个多任务学习的模型。但是它的优化方法？

















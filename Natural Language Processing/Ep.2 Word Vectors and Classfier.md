#NLP 

你发现，Bag of Words 模型并不关注周围词的顺序，而只关注周围词是否在中心词附近出现。但是，这一个粗略的模型已经可以实现很好的效果。

### 词向量的学习
接下来，我们将介绍如何学习词向量模型中的参数。一种常见的方式是使用梯度下降。传统的梯度下降需要遍历全部数据来计算梯度，但是这太慢了，因此我们通常使用的算法是**Stochastic Gradient Descent**
在对词向量进行优化时，我们可能使用一个滑动窗口划过所有的词，仅对这个窗口内的词进行优化。此时，梯度向量可能会十分稀疏。

### 一些模型的变体
- Skip Gram：使用中心词预测外围词
- Continuous Bag of Words：使用一系列的外围词预测中心词
- Negative Sampling ：训练 Logistic Regression 来区分真正的（中心词，周围词）和（中心词，随机词），其目标是最大化：
$$
J_{t}(\theta) = \log \sigma(u_{o}^{T} v_{c}) + \sum_{i} \mathrm{E}_{j \sim P(w)} \log \sigma ( - u_{o}^{T} v_{c})
$$
第一项是正确的（中心词，周围词）；第二项是随机选择的周围词
这里有一些小的 Trick，例如，采样概率并不是这个词出现的真实概率，而是：
$$
P(w) = \frac{U(w)^{\frac{4}{3}} }{Z}
$$
这可能使得语料库中不太出现的词更多地被采样。

### 共现向量
基于共现矩阵，实际上我们可以构造词向量（这是另一个有趣的思路）：
![[6d83b8b97e7a0af2e99ac34c02e666b.jpg|400]]
但是，这种词向量有一些缺点，例如，这种词向量会随着语料库的增大而变长。
因此，我们通常可以通过一些降维手段来将它们打压到固定的维度
另外的一些缺点是，某些太常用的单词将会“污染”这个矩阵，这里我们可以限制最大计数、对计数取 $\log$，通过这些手段来处理问题

这里有一个关键点：我们之前演示过使用词向量模型进行类比任务，这种任务通过将一个词加上一个向量 $v$（这里称作"Meaning Components"）来实现。我们想说的是，这个成分通常编码在共现概率的比例中。
我们可以看这个例子：
![[ad061a8353b5037e747da74261be113.jpg|400]]
在这里，我们要进行的类比是“冰之于蒸汽正如固体之于气体”，那么，我们计算第三行的比例。如果词 $x$ 只与“ice”或者"steam"共同出现，那么这个比例就会偏离 1. 这样，我们可以发现，“soild”可以和"ice"进行类比，"gas"可以和"steam“进行类比
接下来，我们的词向量模型应该捕捉到这一点。我们使用如下的模型：
$$
w_{i} \cdot  w_{j}  = \log P(i|j)
$$
即：词向量的点积代表了共现概率的对数
$$
w_{x}(w_{a} - w_{b} ) = \log \frac{P(x|a)}{P(x|b)}
$$
这样，如果词 $x$ 和 $a$ 的共现率相当高，而和 $b$ 相当低，那么 $w_{x}$ 向量就会靠近 $w_{a}$ 而远离 $w_{b}$。
通过这种方式，我们可以发展出 Glove 算法，其目标函数为：
$$
J = \sum_{i,j} f(X_{ij}) (w_{i}^{T} w_{j} - \log X_{ij})
$$
$f(\cdot)$ 是一个截断的线性函数，防止共现次数太多的词对训练产生影响。

### 如何评估词向量？
- 内在 (Intrinsic)评估：在一个特定的任务上评估（e.g.类比任务）
- 外在 (Extrinsic)评估：在真实的任务上评估，观察准确率的变化 (e.g.命名实体识别)

最后，我们想要说明一点：语言具有多义性，这些词向量如何处理多义词？实际上，我们可能需要为每个含义设置一个词向量。如果你只设置一个，那么这个词向量可能是代表各个意义的词向量的加权求和，权重是出现频率。




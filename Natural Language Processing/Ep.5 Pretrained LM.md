#NLP 

在前面我们已经提到过，在训练图像自编码器（一种以复原图像为训练任务，对图像进行降为的模型）的过程中，我们首先利用受限玻尔兹曼机的思想对网络进行了逐层预训练。在 CV 领域，预训练早已出现：人们发现，网络的底层更可能表征一些可以被多个任务复用的特征；网络的头部则与具体的任务高度相关。

在 NLP 领域，最简单的预训练是**Word Embedding**，但是它是静态的。一种简单的改进方式是使用 Bi-LSTM 对词向量进行“二次编码”
![[Pasted image 20231019142236.png|450]]

![[Pasted image 20231019143326.png|450]]
ELMO 在下游任务中不改变所有预训练好的词向量，只是将这些词向量组合起来。注意：这并没有真正地进行 Fine-Tuning

OpenAI 推出了 GPT 1 ，其原理非常简单：
![[Pasted image 20231019144713.png|450]]
这里，我们将自回归作为了预训练任务。这种模型适合用于自然语言生成。

实际上，我们可以使用上下文信息来预测中间的词。例如，BERT 有两个任务：
- 随机 Mask 15%的位置，预测这些位置上是什么字
- 输入两个句子，随机将第二句替换，预测两句话是否连续
一个句子是形如[CLS]喵喵[MASK]喵喵[SEP]汪汪[MASK]汪[SEP]

但是，BERT 奇怪的地方在于，对于被 MASK 的词汇：
- 80%真的被[MASK]替换
- 10%被留下了
- 10%被替换成其他词
这样做的目的可能是增加噪声？同时防止语言模型过于依赖 [MASK]标记（因为它们在实际中不存在）

[CLS]标记被作为一个句子的起始，从这里输出句子的表示。
两个任务同时训练，模型需要进行多任务学习。

![[Pasted image 20231023155728.png|450]]
这个模型完全使用交叉熵损失（等效于极大似然估计）。
预训练完成后，下游任务可以选择：情感分析、序列标注。做分类的时候，可以使用[CLS]，也可以使用所有位置上的向量进行 Pooling 。

预训练的优势：
- 可以利用到大规模数据，学习到通用的表示； 
- 提供更好的模型初始化，提升模型泛化能力，加速目标任务收敛；
- 可以被看成一种正则化方法，防止模型在小规模标注数据上过拟


到了国内，百度试图将知识注入 LLM：（Mask 短语或者命名实体）
![[Pasted image 20231023163624.png|450]]

BART：Encoder-Decoder 结构，更难的预训练任务：
![[Pasted image 20231023164128.png|450]]

对于 Prompt，其目的是使得下游任务和预训练任务变得一样。例如，我们使用 BERT 来解决一个情感分类任务：
![[Pasted image 20231026135430.png|400]]


### 序列评估任务之一：Tokenization 
- BPE 技术：类似于聚类。将词切开，检查共现次数，将最大共现次数的 subword 合并，直到达到最大容忍的 subword 词表大小就停止
- 最大匹配法：一种机械分词方法
- WordPiece：使用互信息











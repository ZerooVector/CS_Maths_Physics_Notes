#NLP 

在本节中，我们将说明如何使用统计学的方法来研究自然语言处理。
作为一个统计学习的方法，我们通常由三个步骤：
- 建模
- 学习
- 推断
利用统计学，我们可以发现许多有趣现象。例如，词汇的长尾分布。长尾分布会导致一些问题，这会导致某些词的出现概率太低。有的时候，我们会把频率太低的词换成一个 (UNK) 

## Bayesian Theorem 
$$
P(h|D) = \frac{P(D|h)P(h)}{P(D)}
$$
比如，我们可以考虑使用这个模型来求解单词更正问题。其中先验容易统计，似然可以使用编辑距离等方式度量。
- Ockham's Razor ：过拟合？最大熵模型？

### MLE
MLE 不仅可以用于估计生成式模型的参数，还可以用于估计与概率相关的判别式模型的参数。例如，我们估计 Logistic 回归。我们的主要思想是：
$$
w = \arg \max_{w} P(y|x;w)
$$
我们记 $P_{1}(x; w) = P(y = 1|x,w)$ ，那么，我们应当最大化概率：
$$
\prod P = \prod [P_{1}(x;w)]^{y_{1}}[1-P_{1}(x;w)]^{1-y_{1}} 
$$
写出似然，并加负号，我们就得到了：
$$
\mathrm{LOSS}  = - \sum [y_{i}\ln P_{1}(x;w) + (1-y_{i})\ln (1-P_{1}(x;w))]
$$
这个方程就是交叉熵，度量了两个分布之间的距离。我们希望两个分布间的距离尽量小。

### 最大后验估计
我们假定 $w$ 的先验分布是
$$
p(w|v) = \mathcal{N}(w|0,v^{2}I)
$$
推导可以得到岭回归。其实这是很好理解的，既然我们已经将其假设为服从正态分布，显然先验概率最大的地方在 $w = 0$

## Probabilitistic Graph Model 
在贝叶斯模型中，我们有：
$$
P(h,D) = P(h) P(D|h)
$$
在 Naive Bayes 中，我们有：
$$
P(h,D) = P(h) \prod_{i}P(D_{i}|h)
$$
这就是一个最简单的 Bayes 网络。所有节点之间没有连接。
在 Markov 模型中：
$$
P(Q) = \prod_{i} P(q_{i}|q_{i-1})
$$
这时，所有的节点连结成了一条链。
在 Hidden Markov 模型中：
$$
P(O|Q) = P(O|Q) P(Q) 
$$
这个模型通常被用于序列标注、序列评估问题。**注意：可能考** 

#NLP 

信息熵：
$$
H(x)  =  - \sum   p(x) \log p(x)
$$
（考虑霍夫曼编码，这是一种随着信息量改变带宽的编码）

互信息：
$$
I(X,Y) = H(X) - H(X|Y)
$$
（已知信息 $Y$ 后，信息 $X$ 的不确定性的减少程度）
- 小知识：如何使用 Naive Bayes 进行词义消歧->找到与目标词互信息很大的词
交叉熵：
$$
H(x,q) =  - \sum p(x) \log q(x)
$$
这实际上就是一个度量两个分布相似度的手段，可以用于观察我们由一个分布找到另一个的代价
相对熵：
$$
D(p,q) = H(X,q) - H(X)
$$
困惑度：
$$
[\prod_{t-1}^{m} P(w_{t}|w_{t-1})] ^{-1/m}
$$
直观上，我们希望语言模型的“备选词”尽可能地小，那么我们最小化困惑度。

深度学习本质的解释：“信息瓶颈”，我们希望任意压缩自变量 $X$ 中的信息而仍然使得模型保留 $Y$ 的能力
$$
L = -I(Y,X') + \beta I(X,X') 
$$
也就是说，在泛化到下游任务时，模型可以“抛弃”一些东西

- 均匀信息理论：单位语言单元上承载的信息量均匀分布会使得语言处理的认知负担最小
- Beam Search 是符合均匀信息理论的。如果你想增加信息多样性，可以使用采样的方式。







#MLorDS 

***非监督学习学的什么：是 Distribution（分布，也就是数据所包含的统计规律，之后就可以利用贝叶斯的方法将数据区分开），是 Associated Rule（关联规则，也就是挖掘不同事物之间的联系）***

## Clustering ：Learn the Distribution 
In the process of Clustering, we don't know the output, we only dividing the data into clusters according to the similarity between data, and we discovering interesting distributions. 

**三个关键问题：如何度量数据之间的相似性、如何将数据分成不同的几类、如何处理超大规模的或者高维的问题** 

### 度量数据间的相似性
- 最常见的做法是使用距离，距离需要满足：
	- $d (i, j) \ge 0$ 
	- $d (i, i) = 0$
	- $d (i, j) = d(j,i)$
	- $d (i, j) \le d (i, k) + d(k,j)$
常用的可能是 Euclidean , Hamilton, CHebyishev, Minkowski, etc.
- 其他的做法：夹角的余弦值

**实际上，聚类也是有目标的，你要通过搜索的方式找到最优的划分方法，从搜索的方式来看，分为暴力搜索和启发式搜索**

### 启发式聚类系列算法（划分数据的聚类算法）
采用启发式搜索的方式进行聚类的算法有：K-Means, K-medoids
两个算法只有很小的区别，但是它们的核心是相同的：采用 EM 算法，分为 E 步和 M 步。E 步上，要调整每一类的均值；M 步上，要去调节每个点分在哪一类。

#### K-Means
- 随机选择 $k$ 个数据作为每一个 Cluster 中数据的均值
- 将所有的数据划分到离它最近的一簇
- 更新均值
反复迭代以上步骤，直到结果稳定为止。

![[Pasted image 20230313104041.png|450]]

==作业：优化目标是什么？模糊 K-均值聚类又是什么？这需要你自己了解！== 
K-Means 有诸多优缺点：
- 复杂度很低：$O(tkn)$ 的线性复杂度
- 一定可以找到 Local Min，但是也有可能无法达到 Global Min
- 需要指定超参数：$K$ 
- 对于噪声点和初始值敏感！

##### Example : Bag of Visual Words（视觉词典）
- 将每个图象提取特征，然后聚合成 $k$ 类。通过判断图上的点落在哪一类的范围内，我们就可以通过一个 $k$ 维的向量来表征一个图像！
![[Pasted image 20230313105930.png|400]]

#### 解决 k-Means 受噪点影响的问题：PAM (中心周围划分算法)
那么，重点就是如何计算中心点
设有 $n$ 个数据，$k$ 个中心点 $m_{i}$，剩下的是非中心点 $d_{j}$，算法尝试使用非中心点替换中心点

![[Pasted image 20230313111008.png|400]]

左上：用 h 替换 i，替换收益 $C_{ijh}= d (j, h) - d(j,i)$ 
右上：$j$ 与 $i$ 和 $h$ 都不在同一簇 $C_{ijh}= 0$
左下：$j$ 被划分到新的一簇里面 $C_{ijh} = d(j,t)-d(j,i)$ 
那么，每一步的目标是最小化：
$$
C_{ih} = \sum_{j}C_{ijh} 
$$
算法步骤：
- 随机选择 $k$ 个中心点
- 计算每一种替换的目标函数
- 如果目标函数小于 0，执行目标函数最小的替换
不断迭代以上步骤
但是，它的复杂度是 $O((n-k)^{2}kt)$

#### 如何提高 PAM 的效率？

- CLARA：先采样一些点，在这些采样点上运行 PAM （质量受到采样的影响）
- CLARANS：重复采样多次

但是，CKARANS 不是一个纯粹随机的方法，它是一个**图算法** （用图表达搜索的空间）
- 每个节点是聚类的一个解（就是 $k$ 个中心点）
- 每条边是**恰好只有一个中心点不一样的一个解**（类似于“逻辑相邻”的概念）
- 在每轮搜索中，仅仅在一个点的邻域内搜索（实际上是一种爬山算法）
- 

### 层次的聚类算法
数据被一层一层地，自底向上地聚起来，不仅要考虑数据间的相似性，还要考虑簇间相似性
簇间相似性也有几种不同的思路：
- Single-Linkage Algo ：两点间距离的最小值
- Complete-Linkage Algo : 两点间距离的最大值
- Average-Linkage Algo ：距离均值
- 算两个簇的质心间的距离（将簇看作整体）
- 衡量两个簇的分布之间的距离（KL 散度）

一些经典的算法存在问题：层次聚类是复杂度很高的，是有可能“一步错，步步错”的

#### BIRCH 算法
我们不使用具体的数据代表一个簇，而是使用数据的特征
$$
CF(classifiy \ feature) = (N,\vec {LS},SS)
$$
- N 是簇内点的数目
- LS 是所有数据向量的加和
- SS 是向量的模方的和
这个特征有趣的一点是，它是**可加**的。同时，BIRCH 的数据结构使得只需要扫描一遍数据。
需要指定三个超参：B, L 是两个树的参数（B-每个节点最多包含 $B$ 个子节点，L——每个节点的子节点最多有 $L$ 个）；T 是一个聚类阈值，用来判定是不是对节点进行分裂

- 第一步：分裂生成叶节点
- 第二步：将叶节点再（传统的聚类方法）聚类，直到回到根节点

#### CURE 算法
这个算法主要做了以下两种工作：
- 使用几个点，而不是一个点来代表一组数据，**这使得簇的形状得以体现**
- 代表点最初在簇的边缘，而会随着时间逐渐走向簇的中心
- 使用了采样方法
- 人为地将数据拆成几组

- 采样数据点
- 将数据划分成 $p$ 个部分
- 对于每一组，都使用 CURE 的方法进行聚类，生成若干个簇
- 删除奇异点
- 对之前生成的簇再进行聚类
- 将其他数据分到最近的簇

#### CHAMELEON 算法
这是一个**图算法**，或者说，这是一个**谱聚类**的算法
- 构建一个稀疏的图，只要求每个点和 $k$ 个点产生关系
- 使用最小割的方法切割图
- 将小的簇再两两聚合
在计算相似性时，会计算两个簇之间的相似性和簇内的相似性

### 基于密度的聚类 
#### DBSCAN ：一种直观的算法
基本思想：如果以一个点为中心，周围有足够数量的点，那么认为这个点附近是稠密的
![[Pasted image 20230320100846.png|400]]

- 密度直接可达：如果一个点是满足条件的，一个点的 $Eps$ 之内的所有点都是直接可达的点。此时，中心点被称为 core
- 密度可达：
- 密度相连：
DBSCAN 是一种一次扫描的算法，其过程为：
- 对于点 $p$ 找到所有密度可达的点
- 如果 $p$ 是中心点，那么形成一个簇
- 如果是边节点，则不形成一个簇

#### DENCLUE 
首先，DENCLUE 使用 KDE 估计数据的密度。DENCLUE 中的 KDE 使用两种不同的核：
![[Pasted image 20230320102217.png|450]]

![[Pasted image 20230320102637.png]]

DENCLUE 有两种做法：
- 找到 KDE 函数的最高点，将每个点分到离它最近的最高点上
- 任意的簇：给一个阈值，合并低于阈值的所有聚类中心

那么，我们是否存在一种不受数据规模的影响而导致效率变化的算法？
### 基于网格的聚类
#### STING 
![[Pasted image 20230320103456.png|500]]
划分格子，对格子进行聚类
优点：速度快；不足：检测不到对角边缘

#### CLIQUE 
找到一个子空间，使得数据可以明显地被划分
它使用了：低维子空间里不稠密，则高维子空间里也不稠密的原则，通过找到稠密的网格，并以它们为中心进行聚类

## Association Rule Mining 
### 频繁项集和置信度
支持度（Support）：某项交易出现的比例，用于衡量某种物品的集合是否频繁地出现
置信度 （Confidence）：
$$
C(X \Rightarrow Y) = \frac{S(XY)}{S(X)}
$$
一个合理的关联规则应该同时满足 Support 和 Confidence

### 找到频繁项集
使用一种 Apriori 算法——进行剪枝：一个 $k$ 频繁项集的子集必然是 $k$ 频繁的
![[Pasted image 20230320111558.png|400]]

### 从频繁项集找到关联规则
假设频繁项集为 $L$，我们显然要找到 $f \Rightarrow L-f$ 这样的规则。这里也使用了一个剪枝：随着前提的增加，置信度也在增加
![[Pasted image 20230320112153.png|400]]

最后，规则进行合并，使得规则更加简洁，不会相互冲突。



































